# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_gemLearn.ipynb.

# %% auto 0
__all__ = ['def_device', 'remove_leapdays', 'DataLoaders', 'CancelFitException', 'CancelBatchException', 'CancelEpochException',
           'run_cbs', 'Callback', 'CompletionCB', 'to_cpu', 'callback_ctx', 'MetricsCB', 'ProgressCB', 'Learner',
           'OneDCNN', 'MLP', 'GxE', 'to_device', 'TrainCB', 'DeviceCB']

# %% ../nbs/03_gemLearn.ipynb 3
import torch
from torch.utils.data import DataLoader
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.impute import KNNImputer

from sklearn.preprocessing import StandardScaler
from pathlib import Path

import fastcore.all as fc
from collections.abc import Mapping
from pathlib import Path
from operator import attrgetter,itemgetter
from functools import partial
from copy import copy
from contextlib import contextmanager
from warnings import warn
import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn
from torcheval.metrics import MeanSquaredError,Mean, R2Score
from fastprogress import progress_bar,master_bar

from .GEMdataset import *

# %% ../nbs/03_gemLearn.ipynb 4
def remove_leapdays(weather_data):
    """ just a hotfix """
    to_remove = []
    for i in list(set(weather_data['Env'])):
        if (sum(weather_data['Env'] == i)) == 366:
            #get indexes
            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))
    return weather_data.drop(to_remove)

# %% ../nbs/03_gemLearn.ipynb 5
class DataLoaders:
    def __init__(self, *dls): self.train,self.valid = dls[:2]

    @classmethod
    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):
        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds), **kwargs) for ds in dd.values()])

# %% ../nbs/03_gemLearn.ipynb 6
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/03_gemLearn.ipynb 7
def run_cbs(cbs, method_nm, learn=None):
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_nm, None)
        if method is not None: method(learn)

# %% ../nbs/03_gemLearn.ipynb 8
class Callback(): order = 0

# %% ../nbs/03_gemLearn.ipynb 9
class CompletionCB(Callback):
    def before_fit(self, learn): self.count = 0
    def after_batch(self, learn): self.count += 1
    def after_fit(self, learn): print(f'Completed {self.count} batches')

# %% ../nbs/03_gemLearn.ipynb 10
def to_cpu(x):
    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}
    if isinstance(x, list): return [to_cpu(o) for o in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    return x.detach().cpu()

# %% ../nbs/03_gemLearn.ipynb 11
@contextmanager
def callback_ctx(cbmeth, nm):
    try:
        cbmeth(f'before_{nm}')
        yield
        cbmeth(f'after_{nm}')
    except globals()[f'Cancel{nm.title()}Exception']: pass
    finally: cbmeth(f'cleanup_{nm}')

# %% ../nbs/03_gemLearn.ipynb 12
class MetricsCB(Callback):
    def __init__(self, *ms, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()

    def _log(self, d): print(d)
    def before_fit(self, learn): learn.metrics = self
    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]

    def after_epoch(self, learn):
        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
        log['epoch'] = learn.epoch
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)

    def after_batch(self, learn):
        y,_,_ = to_cpu(learn.batch)
        if y.shape == learn.preds.squeeze().shape:
            for m in self.metrics.values(): m.update(to_cpu(learn.preds.squeeze()), y)
            self.loss.update(to_cpu(learn.loss), weight=len(y))
        else:
            pass

# %% ../nbs/03_gemLearn.ipynb 13
class ProgressCB(Callback):
    order = MetricsCB.order+1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []

    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)

    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
    def after_batch(self, learn):
        learn.dl.comment = f'{learn.loss:.3f}'
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.losses.append(learn.loss.item())
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])

# %% ../nbs/03_gemLearn.ipynb 14
class Learner():
    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):
        cbs = fc.L(cbs)
        self.cb_ctx = partial(callback_ctx, self.callback)
        fc.store_attr()

    def one_epoch(self, train):
        self.model.train(train)
        self.dl = self.dls.train if train else self.dls.valid
        with self.cb_ctx('epoch'):
            for self.iter,self.batch in enumerate(self.dl):
                with self.cb_ctx('batch'):
                    self.predict()
                    self.get_loss()
                    if self.training:
                        self.backward()
                        self.step()
                        self.zero_grad()
    
    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):
        cbs = fc.L(cbs)
        # `add_cb` and `rm_cb` were added in lesson 18
        for cb in cbs: self.cbs.append(cb)
        try:
            self.n_epochs = n_epochs
            self.epochs = range(n_epochs)
            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)
            with self.cb_ctx('fit'):
                for self.epoch in self.epochs:
                    if train: self.one_epoch(True)
                    if valid: torch.no_grad()(self.one_epoch)(False)
        finally:
            for cb in cbs: self.cbs.remove(cb)

    def __getattr__(self, name):
        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)
        raise AttributeError(name)

    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)
    
    @property
    def training(self): return self.model.training

# %% ../nbs/03_gemLearn.ipynb 15
class OneDCNN(torch.nn.Module):
    def __init__(self):
        super(OneDCNN, self).__init__()

        # Define the layers
        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(146000, 100)
        self.fc2 = nn.Linear(100, 100)
        self.dropout = nn.Dropout(p=0.5)
        self.relu = nn.ReLU()

    def forward(self, x):
        # Apply the layers
        x =  x.reshape(x.shape[0],1,50)
        x = self.conv1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.conv3(x)
        x = self.relu(x)
        x = self.relu(x)
        x = x.view(x.shape[0],x.shape[1]*x.shape[2])
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)

        return x
    

class MLP(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x


class GxE(torch.nn.Module):
    def __init__(self,mlp, cnn):
        super(GxE, self).__init__()
        self.MLP = mlp
        self.OneDCNN = cnn
        self.fc1 = nn.Linear(200,100)
        self.fc2 = nn.Linear(100,1)
        
    def forward(self,x):
        g,w = x
#        g = g.reshape(batch_size,100)
        
        
        g = self.MLP(g)
        w = self.OneDCNN(w)
        
        x= torch.concat((g,w),axis=1)
        x = self.fc1(x)
        x = torch.relu(x)
        #print(x.shape)
        x = self.fc2(x)
        return x

# %% ../nbs/03_gemLearn.ipynb 16
def_device = 'cuda' if torch.cuda.is_available() else 'cpu'
def to_device(x, device=def_device):
    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}
    return type(x)(o.type(torch.float32).to(device) for o in x)

# %% ../nbs/03_gemLearn.ipynb 17
class TrainCB(Callback):
    def predict(self, learn):
        learn.preds = learn.model(learn.batch[1])
    def get_loss(self, learn):
        #print(learn.preds.squeeze().shape, learn.batch[0].shape)
        learn.loss = learn.loss_func(learn.preds.squeeze(), learn.batch[0])
    def backward(self, learn): learn.loss.backward()
    def step(self, learn): learn.opt.step()
    def zero_grad(self, learn): learn.opt.zero_grad()

# %% ../nbs/03_gemLearn.ipynb 18
def_device = 'cuda' if torch.cuda.is_available() else 'cpu'
class DeviceCB(Callback):
    def __init__(self, device=def_device): fc.store_attr()
    def before_fit(self, learn):
        learn.model.to(self.device)
    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)

[
  {
    "objectID": "gemlearn-copy1.html",
    "href": "gemlearn-copy1.html",
    "title": "models",
    "section": "",
    "text": "import torch\n\nclass MLP(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n        super().__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        # Create a list of linear layers, with the correct input and output dimensions\n        self.layers = torch.nn.ModuleList([torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers - 1)])\n        self.layers.append(torch.nn.Linear(hidden_dim, output_dim))\n\n        # Initialize the weights and biases of the linear layers using the Xavier initialization method\n        for layer in self.layers:\n            torch.nn.init.xavier_uniform_(layer.weight)\n            if layer.bias is not None:\n                torch.nn.init.zeros_(layer.bias)\n\n    def forward(self, x):\n        # Apply the dropout layer to the input\n        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n\n        # Iterate through the linear layers, applying each one to the input\n        for layer in self.layers:\n            x = layer(x)\n            x = torch.nn.functional.relu(x)\n            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n\n        return x"
  },
  {
    "objectID": "snpcompression.html",
    "href": "snpcompression.html",
    "title": "snpCompression",
    "section": "",
    "text": "import allel\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm as tqdm\nimport os\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom hybridpredictmaize22.GEMdataset import *\n\n\nsource\n\nvcf2numpy\n\n vcf2numpy (vcf:str, chr:int)\n\nin: vcf -> file.vcf chr -> chr (to be pulled)\nreturns the allele dosage (0, 0.5, 1) for all hybrids\n\nsource\n\n\nSNP_PCA\n\n SNP_PCA (vcfFile, foldername='./PC/', pc=10)\n\nInput: foldername : storage location of npy arrays pc (int: # of pcs per chromosome) vcfFile (path)\n\nsource\n\n\nSNP_evensample\n\n SNP_evensample (snps=50)\n\nInput: foldername : storage location of npy arrays snps (int: # of snps sampled per chromosome) vcfFile (path)\n\ndef collect_snps(method):\n    \"\"\"\n    Input\n        method: a Path(PosixPath) to directory containing npy arrays for each chr\n    Output\n        tuple(hybrid ids, snp matrix)\n    \"\"\"\n    for c,chr in enumerate(method.iterdir()):\n        if c == 0:\n            strains,snp_data = np.load(chr,allow_pickle=True)\n        else:\n            strains, snps = np.load(chr, allow_pickle=True)\n            snp_data = np.vstack((snp_data,snps))\n\n    return strains,snp_data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hybridpredictmaize22",
    "section": "",
    "text": "Repo for analysis of GEM prediction for maize yield"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "hybridpredictmaize22",
    "section": "How to use",
    "text": "How to use\nA demo of the library specifically for this dataset\n\nYield, Genotype, Weather = prep_gem_data(trait_csv='./data/Training_Data/1_Training_Trait_Data_2014_2021.csv',\n                weather_csv = './data/Training_Data/4_Training_Weather_Data_2014_2021.csv',\n                snp_folder = './data/snpCompress/PCS_10/'\n               )\n\n136012 plots with 26 features\n77431 daily weather measurements with 18 features\n4928 hybrids\n\n\n\nYield\n\n\n\n\n\n  \n    \n      \n      index\n      Env\n      Year\n      Field_Location\n      Experiment\n      Replicate\n      Block\n      Plot\n      Range\n      Pass\n      ...\n      Stand_Count_plants\n      Pollen_DAP_days\n      Silk_DAP_days\n      Plant_Height_cm\n      Ear_Height_cm\n      Root_Lodging_plants\n      Stalk_Lodging_plants\n      Yield_Mg_ha\n      Grain_Moisture\n      Twt_kg_m3\n    \n  \n  \n    \n      0\n      0\n      DEH1_2014\n      2014\n      DEH1\n      G2F_2014_15\n      1\n      1\n      1\n      1.0\n      1.0\n      ...\n      56.0\n      63.0\n      67.0\n      213.00\n      79.00\n      0.0\n      0.0\n      5.721725\n      20.8\n      706.664693\n    \n    \n      1\n      1\n      DEH1_2014\n      2014\n      DEH1\n      G2F_2014_15\n      1\n      1\n      2\n      1.0\n      2.0\n      ...\n      54.0\n      61.0\n      63.0\n      286.00\n      172.00\n      0.0\n      0.0\n      11.338246\n      25.8\n      693.792841\n    \n    \n      2\n      2\n      DEH1_2014\n      2014\n      DEH1\n      G2F_2014_15\n      1\n      1\n      3\n      1.0\n      3.0\n      ...\n      60.0\n      63.0\n      65.0\n      239.00\n      92.00\n      0.0\n      4.0\n      6.540810\n      20.8\n      698.941582\n    \n    \n      3\n      3\n      DEH1_2014\n      2014\n      DEH1\n      G2F_2014_15\n      1\n      1\n      4\n      1.0\n      4.0\n      ...\n      59.0\n      61.0\n      63.0\n      242.00\n      118.00\n      0.0\n      0.0\n      10.366857\n      23.7\n      711.813434\n    \n    \n      4\n      4\n      DEH1_2014\n      2014\n      DEH1\n      G2F_2014_15\n      1\n      1\n      5\n      1.0\n      5.0\n      ...\n      58.0\n      63.0\n      65.0\n      211.00\n      92.00\n      0.0\n      0.0\n      10.908814\n      19.4\n      743.993065\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      129303\n      136007\n      WIH3_2021\n      2021\n      WIH3\n      G2F_2020_21_PHP02\n      2\n      25\n      496\n      19.0\n      9.0\n      ...\n      80.0\n      75.0\n      76.0\n      251.67\n      123.33\n      NaN\n      NaN\n      9.972527\n      16.9\n      698.941582\n    \n    \n      129304\n      136008\n      WIH3_2021\n      2021\n      WIH3\n      G2F_2020_21_PHP02\n      2\n      25\n      497\n      19.0\n      8.0\n      ...\n      65.0\n      81.0\n      90.0\n      303.33\n      148.33\n      1.0\n      NaN\n      9.160941\n      23.7\n      709.239064\n    \n    \n      129305\n      136009\n      WIH3_2021\n      2021\n      WIH3\n      G2F_2020_21_PHP02\n      2\n      25\n      498\n      19.0\n      7.0\n      ...\n      69.0\n      76.0\n      79.0\n      301.67\n      150.00\n      NaN\n      NaN\n      9.256348\n      19.5\n      732.408398\n    \n    \n      129306\n      136010\n      WIH3_2021\n      2021\n      WIH3\n      G2F_2020_21_PHP02\n      2\n      25\n      499\n      19.0\n      6.0\n      ...\n      81.0\n      78.0\n      79.0\n      293.33\n      165.00\n      1.0\n      5.0\n      11.504058\n      19.1\n      692.505656\n    \n    \n      129307\n      136011\n      WIH3_2021\n      2021\n      WIH3\n      G2F_2020_21_PHP02\n      2\n      25\n      500\n      19.0\n      5.0\n      ...\n      81.0\n      76.0\n      77.0\n      291.67\n      165.00\n      NaN\n      9.0\n      11.618923\n      19.5\n      696.367212\n    \n  \n\n129308 rows × 27 columns\n\n\n\n\nGenotype\n\n(array(['2369/DK3IIH6', '2369/PHN82', '2369/PHZ51', ...,\n        'Z038E0057/DK3IIH6', 'Z038E0057/LH162', 'Z038E0057/PHZ51'],\n       dtype=object),\n array([[-0.00898487, -0.00889737, -0.00985531, ..., -0.00912366,\n         -0.00703719, -0.00999637],\n        [ 0.00958669,  0.01123948, -0.02311905, ...,  0.00941995,\n         -0.00142499, -0.02322749],\n        [ 0.01249667,  0.00819269, -0.0079599 , ...,  0.01258645,\n          0.00026784, -0.00785536],\n        ...,\n        [-0.01473902, -0.0097304 , -0.00344457, ..., -0.01584803,\n         -0.00909118, -0.00434402],\n        [ 0.00850316,  0.0033058 ,  0.00246347, ...,  0.00928889,\n         -0.00240442,  0.00304883],\n        [-0.00583694, -0.00337831, -0.00102845, ..., -0.00549591,\n         -0.00374846, -0.000797  ]]))\n\n\n\nWeather\n\n\n\n\n\n  \n    \n      \n      index\n      Env\n      Date\n      QV2M\n      T2MDEW\n      PS\n      RH2M\n      WS2M\n      GWETTOP\n      ALLSKY_SFC_SW_DWN\n      ALLSKY_SFC_PAR_TOT\n      T2M_MAX\n      T2M_MIN\n      T2MWET\n      GWETROOT\n      T2M\n      GWETPROF\n      ALLSKY_SFC_SW_DNI\n      PRECTOTCORR\n      Year\n    \n  \n  \n    \n      0\n      0\n      ARH1_2016\n      20160101\n      3.54\n      -0.78\n      102.34\n      77.00\n      2.15\n      0.84\n      8.21\n      41.96\n      7.80\n      -0.70\n      1.15\n      0.83\n      3.08\n      0.80\n      5.96\n      0.00\n      2016\n    \n    \n      1\n      1\n      ARH1_2016\n      20160102\n      3.23\n      -1.91\n      102.04\n      74.62\n      1.49\n      0.84\n      11.28\n      55.13\n      10.15\n      -3.10\n      0.42\n      0.83\n      2.74\n      0.80\n      16.13\n      0.00\n      2016\n    \n    \n      2\n      2\n      ARH1_2016\n      20160103\n      4.09\n      1.05\n      101.59\n      80.69\n      1.95\n      0.84\n      9.78\n      49.21\n      12.39\n      -1.29\n      2.72\n      0.83\n      4.38\n      0.80\n      18.36\n      0.00\n      2016\n    \n    \n      3\n      3\n      ARH1_2016\n      20160104\n      2.87\n      -3.49\n      102.24\n      79.88\n      3.45\n      0.84\n      7.35\n      35.66\n      4.56\n      -4.00\n      -1.79\n      0.83\n      -0.09\n      0.80\n      10.87\n      0.00\n      2016\n    \n    \n      4\n      4\n      ARH1_2016\n      20160105\n      2.81\n      -3.64\n      102.37\n      78.81\n      1.95\n      0.84\n      13.00\n      62.04\n      6.94\n      -4.59\n      -1.74\n      0.82\n      0.16\n      0.80\n      27.02\n      0.00\n      2016\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      63595\n      77361\n      TXH4_2019\n      20191023\n      4.03\n      -0.77\n      90.26\n      34.19\n      2.94\n      0.33\n      18.76\n      94.70\n      26.10\n      7.84\n      7.62\n      0.35\n      16.00\n      0.38\n      33.63\n      0.00\n      2019\n    \n    \n      63596\n      77362\n      TXH4_2019\n      20191024\n      4.09\n      -0.46\n      90.98\n      64.88\n      6.20\n      0.34\n      3.61\n      20.41\n      11.37\n      0.13\n      2.63\n      0.35\n      5.73\n      0.38\n      2.57\n      0.30\n      2019\n    \n    \n      63597\n      77363\n      TXH4_2019\n      20191025\n      2.56\n      -6.35\n      91.14\n      46.31\n      3.95\n      0.34\n      19.07\n      93.01\n      13.57\n      -0.67\n      -0.52\n      0.36\n      5.31\n      0.38\n      35.17\n      0.00\n      2019\n    \n    \n      63598\n      77364\n      TXH4_2019\n      20191026\n      2.81\n      -5.44\n      90.00\n      33.81\n      3.30\n      0.34\n      18.52\n      91.70\n      24.73\n      0.55\n      3.09\n      0.35\n      11.62\n      0.38\n      34.00\n      0.25\n      2019\n    \n    \n      63599\n      77365\n      TXH4_2019\n      20191027\n      4.09\n      -0.48\n      90.14\n      49.19\n      3.41\n      0.34\n      18.39\n      91.51\n      17.08\n      3.90\n      4.76\n      0.35\n      9.99\n      0.38\n      34.26\n      0.00\n      2019\n    \n  \n\n63600 rows × 20 columns\n\n\n\n\n#Create a GEM dataset\ntestYear = 2019\n\ngem = GemDataset(\nW=WT(Weather,testYear=testYear),\nY=ST(Yield,testYear=testYear),\nG=Genotype,)\n\n\ntr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\nte_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\ntr_dl = DataLoader(tr_ds, batch_size=4)\nte_dl = DataLoader(te_ds, batch_size=4)\ndls = DataLoaders(tr_dl,te_dl)\n\n\n#data prepped for a ML model\nnext(iter(tr_dl))\n\n[tensor([[-0.3674, -0.4245, -0.1066, -0.3176, -1.1152, -0.1853, -1.2184],\n         [-0.5100, -0.6573, -0.5712,  1.5143,  2.4304, -0.2731,  0.7274],\n         [-0.0822, -0.4245, -0.3389,  0.3349, -0.6196, -0.2379, -0.9346],\n         [-0.1535, -0.6573, -0.5712,  0.4102,  0.3717, -0.1502,  0.3909]],\n        dtype=torch.float64),\n tensor([[-1.2537e-02, -1.9672e-02,  1.6811e-03,  3.2169e-04, -8.1379e-03,\n           7.4324e-03,  4.6273e-02,  1.8587e-02,  2.1311e-02,  1.0904e-02,\n          -1.2763e-02, -1.0796e-02,  6.2732e-03,  4.2459e-03,  1.9839e-02,\n           1.1376e-03,  2.0380e-02, -4.4120e-02,  9.0880e-03,  2.6153e-02,\n          -1.4030e-02,  4.1546e-03,  1.1018e-02,  5.5368e-04, -3.3207e-02,\n          -4.7246e-02, -3.6306e-02, -3.4325e-03, -1.3906e-02,  7.7288e-04,\n          -1.0862e-02, -1.0856e-02,  7.2510e-03,  1.8728e-02, -6.9962e-03,\n           3.3973e-02, -3.2979e-03,  2.8660e-02, -3.0160e-02,  2.5519e-02,\n          -1.3406e-02,  8.9785e-03, -8.0753e-03, -1.8591e-02, -2.9577e-03,\n          -1.4919e-03,  6.0854e-03,  2.5787e-02, -1.3677e-02,  3.6430e-02,\n          -1.0911e-02, -5.0369e-03, -8.4115e-03, -1.1318e-02, -4.8760e-02,\n           4.5473e-03,  1.9822e-02,  1.0351e-02,  1.1252e-02,  1.6777e-02,\n          -1.3320e-02,  6.2947e-03, -8.4701e-03, -3.9584e-03,  1.5415e-03,\n          -5.2472e-02,  1.4938e-02,  2.7120e-02,  1.2685e-02,  1.8382e-02,\n          -1.5683e-02, -3.4921e-03, -1.4228e-03,  1.3622e-02, -3.2181e-02,\n          -1.1613e-02,  4.1971e-02, -7.4261e-03, -1.3409e-02,  8.9346e-05,\n          -1.5654e-02,  5.9343e-03,  1.7004e-02, -2.2572e-03,  2.4609e-02,\n          -2.4282e-02,  2.2182e-02,  1.4706e-02,  3.8526e-02, -2.4709e-02,\n          -1.1599e-02,  1.5737e-02,  1.1769e-02, -2.3031e-02, -5.9856e-04,\n           3.2283e-03, -3.2635e-02, -2.6049e-02, -4.3127e-03,  1.8938e-02],\n         [-1.1498e-02, -1.8355e-02,  8.5742e-04,  1.6298e-03, -6.1542e-03,\n           5.7250e-03,  4.5892e-02,  2.3061e-02,  1.9783e-02,  9.0742e-03,\n          -1.3859e-02, -1.3625e-02,  7.1786e-03,  6.7976e-03,  2.3107e-02,\n          -3.2542e-04,  2.1017e-02, -4.9267e-02,  9.9663e-03,  2.7567e-02,\n          -1.3052e-02,  4.6522e-03,  9.0669e-03, -4.5637e-03, -3.1818e-02,\n          -4.2016e-02, -3.3461e-02, -5.7945e-04, -1.5299e-02,  4.5791e-03,\n          -1.0077e-02, -1.0859e-02,  6.4589e-03,  1.7558e-02, -5.9192e-03,\n           3.0332e-02,  1.7486e-04,  2.4698e-02, -2.8495e-02,  2.5888e-02,\n          -1.1609e-02,  7.5121e-03, -7.7189e-03, -1.8198e-02, -2.8820e-03,\n           9.1731e-04,  3.9233e-03,  2.1096e-02, -1.3771e-02,  3.2179e-02,\n          -1.2340e-02, -6.4690e-03, -8.6480e-03, -1.2456e-02, -5.1094e-02,\n           5.6756e-03,  1.9305e-02,  1.2972e-02,  6.3360e-03,  1.8971e-02,\n          -1.3927e-02,  7.8069e-03, -1.0485e-02, -2.9810e-03,  2.8357e-03,\n          -4.6808e-02,  1.1290e-02,  2.2743e-02,  1.2329e-02,  1.1519e-02,\n          -1.5996e-02,  4.1565e-05, -3.0390e-03,  1.3846e-02, -3.2027e-02,\n          -1.7381e-02,  4.0789e-02, -1.0124e-02, -1.5366e-02, -2.6635e-03,\n          -1.3908e-02,  7.7443e-03,  1.6824e-02, -4.3095e-04,  1.8993e-02,\n          -2.2203e-02,  1.9662e-02,  8.6041e-03,  3.5227e-02, -2.4014e-02,\n          -1.7380e-02,  2.1479e-02,  1.4567e-02, -3.7964e-02,  3.1776e-03,\n           3.7787e-03, -4.2635e-02, -2.5507e-02,  4.2993e-03,  1.4310e-02],\n         [-1.3517e-02, -2.6199e-02, -1.7213e-03, -4.3111e-03, -7.1296e-03,\n           1.1442e-02,  5.1067e-02,  3.0887e-02,  2.9725e-02,  1.9873e-02,\n          -1.2706e-02, -1.1936e-02,  8.3235e-03,  8.0560e-03,  2.4286e-02,\n           7.7760e-04,  2.4337e-02, -3.8809e-02,  1.3299e-02,  3.0288e-02,\n          -1.3576e-02,  4.7275e-03,  1.0127e-02, -9.9074e-04, -3.3696e-02,\n          -4.6280e-02, -3.8001e-02, -4.0238e-03, -1.0919e-02, -2.6381e-04,\n          -1.4135e-02, -7.6515e-03,  6.0889e-03,  1.7242e-02, -8.7120e-03,\n           3.5676e-02,  1.9086e-02,  3.7318e-02, -1.7984e-02,  1.8168e-02,\n          -1.1958e-02,  9.4653e-03, -8.6502e-03, -1.6374e-02, -4.7912e-03,\n           3.0692e-04,  5.6347e-03,  2.1774e-02, -1.5386e-02,  3.5806e-02,\n          -1.0881e-02, -2.4079e-03, -9.3225e-03, -1.6402e-02, -5.1255e-02,\n           7.9421e-03,  1.9299e-02,  1.6439e-02,  9.5289e-03,  1.4894e-02,\n          -1.0674e-02,  1.0870e-03, -1.0235e-02, -3.6468e-03,  8.1990e-03,\n          -4.2869e-02,  1.5734e-02,  2.7203e-02,  1.3128e-02,  2.4335e-02,\n          -1.5203e-02,  1.7994e-05, -2.7458e-03,  1.3447e-02, -2.9427e-02,\n          -1.6841e-02,  4.0346e-02, -1.0675e-02, -1.6819e-02, -8.6676e-05,\n          -1.0703e-02,  3.8087e-03,  1.0059e-02, -5.5048e-03,  2.1881e-02,\n          -2.7831e-02,  2.1065e-02,  8.5539e-03,  2.1192e-02, -1.9130e-02,\n          -1.0548e-02,  1.3795e-02,  9.3239e-03, -1.9895e-02,  2.3669e-03,\n          -2.8407e-03, -2.7566e-02, -1.9302e-02, -3.4307e-03,  1.4496e-02],\n         [-1.2148e-02, -2.0114e-02, -6.0790e-04, -1.5369e-03, -6.8070e-03,\n           4.5015e-03,  4.6385e-02,  2.7471e-02,  2.4815e-02,  1.0668e-02,\n          -1.0498e-02, -1.2229e-02,  6.4627e-03,  5.9598e-03,  2.4430e-02,\n           1.4147e-04,  1.8871e-02, -3.7979e-02,  1.3362e-02,  2.9211e-02,\n          -1.2745e-02,  4.9000e-03,  1.3101e-02, -8.2709e-06, -3.3810e-02,\n          -4.7337e-02, -3.6354e-02,  1.0522e-04, -1.5212e-02,  4.3197e-03,\n          -1.6717e-02, -1.0779e-02,  1.1230e-02,  2.2515e-02, -7.4418e-03,\n           3.7639e-02,  1.0912e-02,  4.0101e-02, -1.9472e-02,  2.5783e-02,\n          -1.5565e-02,  8.0153e-03, -1.1386e-02, -2.2879e-02,  5.4181e-04,\n          -3.8821e-04,  4.5219e-03,  2.6818e-02, -6.1364e-03,  4.0033e-02,\n          -1.0904e-02, -3.5081e-03, -6.7625e-03, -1.0634e-02, -4.6673e-02,\n           8.1098e-03,  1.7810e-02,  8.5743e-03,  9.9489e-03,  1.0720e-02,\n          -1.1356e-02,  1.0095e-03, -9.0152e-03, -3.6597e-03,  7.7493e-03,\n          -4.3660e-02,  1.6484e-02,  2.5366e-02,  9.0429e-03,  2.3866e-02,\n          -1.3799e-02,  1.3099e-03, -2.3157e-03,  4.5078e-03, -3.6642e-02,\n          -9.7869e-03,  3.1896e-02,  4.8354e-03, -1.4272e-02, -1.5929e-02,\n          -1.4628e-02,  6.7334e-03,  1.4995e-02,  7.1268e-04,  2.1025e-02,\n          -2.3548e-02,  2.2356e-02,  1.1028e-02,  3.1222e-02, -2.2854e-02,\n          -1.6417e-02,  1.9073e-02,  1.3591e-02, -3.9319e-02,  2.8348e-03,\n           8.2323e-03, -4.3892e-02, -2.0638e-02,  5.3391e-03,  1.1821e-02]],\n        dtype=torch.float64),\n tensor([[[0.1469, 0.5021, 0.9387,  ..., 0.7465, 0.4556, 0.0000],\n          [0.1841, 0.5590, 0.8707,  ..., 0.7465, 0.0192, 0.0401],\n          [0.0692, 0.3544, 0.9083,  ..., 0.7465, 0.4701, 0.0078],\n          ...,\n          [0.2848, 0.6643, 0.8694,  ..., 0.4507, 0.6161, 0.0000],\n          [0.2618, 0.6389, 0.8576,  ..., 0.4507, 0.6889, 0.0000],\n          [0.2472, 0.6291, 0.8960,  ..., 0.4507, 0.6494, 0.0000]],\n \n         [[0.1469, 0.5021, 0.9387,  ..., 0.7465, 0.4556, 0.0000],\n          [0.1841, 0.5590, 0.8707,  ..., 0.7465, 0.0192, 0.0401],\n          [0.0692, 0.3544, 0.9083,  ..., 0.7465, 0.4701, 0.0078],\n          ...,\n          [0.2848, 0.6643, 0.8694,  ..., 0.4507, 0.6161, 0.0000],\n          [0.2618, 0.6389, 0.8576,  ..., 0.4507, 0.6889, 0.0000],\n          [0.2472, 0.6291, 0.8960,  ..., 0.4507, 0.6494, 0.0000]],\n \n         [[0.1469, 0.5021, 0.9387,  ..., 0.7465, 0.4556, 0.0000],\n          [0.1841, 0.5590, 0.8707,  ..., 0.7465, 0.0192, 0.0401],\n          [0.0692, 0.3544, 0.9083,  ..., 0.7465, 0.4701, 0.0078],\n          ...,\n          [0.2848, 0.6643, 0.8694,  ..., 0.4507, 0.6161, 0.0000],\n          [0.2618, 0.6389, 0.8576,  ..., 0.4507, 0.6889, 0.0000],\n          [0.2472, 0.6291, 0.8960,  ..., 0.4507, 0.6494, 0.0000]],\n \n         [[0.1469, 0.5021, 0.9387,  ..., 0.7465, 0.4556, 0.0000],\n          [0.1841, 0.5590, 0.8707,  ..., 0.7465, 0.0192, 0.0401],\n          [0.0692, 0.3544, 0.9083,  ..., 0.7465, 0.4701, 0.0078],\n          ...,\n          [0.2848, 0.6643, 0.8694,  ..., 0.4507, 0.6161, 0.0000],\n          [0.2618, 0.6389, 0.8576,  ..., 0.4507, 0.6889, 0.0000],\n          [0.2472, 0.6291, 0.8960,  ..., 0.4507, 0.6494, 0.0000]]],\n        dtype=torch.float64)]"
  },
  {
    "objectID": "gemlearn.html",
    "href": "gemlearn.html",
    "title": "GEMlearn",
    "section": "",
    "text": "source\n\nremove_leapdays\n\n remove_leapdays (weather_data)\n\njust a hotfix\n\nsource\n\n\nDataLoaders\n\n DataLoaders (*dls)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nCancelEpochException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelBatchException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelFitException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nrun_cbs\n\n run_cbs (cbs, method_nm, learn=None)\n\n\nsource\n\n\nCallback\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nCompletionCB\n\n CompletionCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\ncallback_ctx\n\n callback_ctx (cbmeth, nm)\n\n\nsource\n\n\nMetricsCB\n\n MetricsCB (*ms, **metrics)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nProgressCB\n\n ProgressCB (plot=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLearner\n\n Learner (model, dls=(0,), loss_func=<function mse_loss>, lr=0.1,\n          cbs=None, opt_func=<class 'torch.optim.sgd.SGD'>)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nGxE\n\n GxE (mlp, cnn)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nMLP\n\n MLP (input_size, hidden_size, output_size)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nOneDCNN\n\n OneDCNN ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nto_device\n\n to_device (x, device='cpu')\n\n\nsource\n\n\nTrainCB\n\n TrainCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "gemdataset.html",
    "href": "gemdataset.html",
    "title": "GEMdataset",
    "section": "",
    "text": "source\n\nclip_weatherdays\n\n clip_weatherdays (weather_table, days_of_year=300)\n\nInput : Pandas Table w/ weather data Output : Pandas Table w/ # of days for each environment clipped (from beginning of year.\n\nsource\n\n\ncollect_snps\n\n collect_snps (method)\n\nInput method: a Path(PosixPath) to directory containing npy arrays for each chr Output tuple(hybrid ids, snp matrix)\n\nsource\n\n\nprep_gem_data\n\n prep_gem_data (trait_csv, weather_csv, snp_folder)\n\n\nsource\n\n\nWT\n\n WT (weather_data, testYear)\n\nA class which will hold the weather data for the entire dataset for training purposes\ninit weather_data -> pandas table testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n\nsource\n\n\nYT\n\n YT (yield_data, testYear)\n\nA class which will hold the yield data for the entire dataset for training purposes\ninit yield_data -> pandas table testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n\nsource\n\n\nST\n\n ST (yield_data, testYear)\n\nA class which will hold the secondary trait data for the entire dataset for pre-training purposes\ninit yield_data -> pandas table testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n\nsource\n\n\nGemDataset\n\n GemDataset (W, Y, G)\n\nPytorch Dataset which can be used with dataloaders for simple batching during training loops"
  }
]
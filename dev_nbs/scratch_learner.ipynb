{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp GEMlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMdataset\n",
    "\n",
    "> This package will hold the GxExM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "#| hide\n",
    "from hybridpredictmaize22.GEMdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from warnings import warn\n",
    "\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import progress_bar,master_bar\n",
    "from torcheval.metrics import MeanSquaredError,Mean, R2Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def remove_leapdays(weather):\n",
    "    \"\"\" just a hotfix \"\"\"\n",
    "    to_remove = []\n",
    "    for i in list(set(weather_data['Env'])):\n",
    "        if (sum(weather_data['Env'] == i)) == 366:\n",
    "            #get indexes\n",
    "            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))\n",
    "    return weather_data.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = {\n",
    "'test_split':2019,\n",
    "'path_snps':Path('data/snpCompress/'),\n",
    "'data_path':Path('data/'),\n",
    "'path_train_weatherTable':Path('data/Training_Data/4_Training_Weather_Data_2014_2021.csv'),\n",
    "'path_train_yieldTable':Path('data/Training_Data/1_Training_Trait_Data_2014_2021.csv'),\n",
    "'snp_compression':'PCS_50',\n",
    "'batch_size':64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('data/snpCompress/')\n",
    "data_path = Path('data/')\n",
    "path_train_weatherTable =data_path/'Training_Data/4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'Training_Data/1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_50'\n",
    "batch_size = 1\n",
    "\n",
    "def setup_data():\n",
    "    \n",
    "    snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "    yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "    weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "    \n",
    "    yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "    #yield_data = yield_data.reset_index()\n",
    "    weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "    #removes yield data where no weather data\n",
    "    setYield = set(yield_data['Env'])\n",
    "    setWeather = set(weather_data['Env'])\n",
    "    only_yield = setYield - setWeather\n",
    "    only_weather = setWeather - setYield\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "    #removes yield data where no genotype data\n",
    "    setSNP = set(snp_data[0])\n",
    "    setYield = set(yield_data['Hybrid'])\n",
    "    only_yield = setYield - setSNP\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "    weather_data = remove_leapdays(weather_data)\n",
    "    #weather_data = weather_data.reset_index()\n",
    "    #yield_data=yield_data.sample(frac=1)\n",
    "    yield_data = yield_data.reset_index()\n",
    "    \n",
    "    #Create a GEM dataset\n",
    "    gem = GEM(test_split)\n",
    "    gem.Y = YT(yield_data, test_split)\n",
    "    gem.W = WT(weather_data, test_split)\n",
    "    gem.SNP = snp_data\n",
    "    \n",
    "    tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "    te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "    \n",
    "    tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return tr_dataloader, te_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('column_name', axis=1, inplace=True)\n",
    "\n",
    "yield_data.drop('index',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "#Create a GEM dataset\n",
    "gem = GEM(test_split)\n",
    "\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dls = (tr_dataloader, te_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create a list of linear layers, with the correct input and output dimensions\n",
    "        self.layers = torch.nn.ModuleList([torch.nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers - 1)])\n",
    "        self.layers.append(torch.nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        # Initialize the weights and biases of the linear layers using the Xavier initialization method\n",
    "        for layer in self.layers:\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "            if layer.bias is not None:\n",
    "                torch.nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the dropout layer to the input\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Iterate through the linear layers, applying each one to the input\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [04:08,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "predslist = []\n",
    "model = MLP(500,50,1, num_layers=5)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "loss_func = F.mse_loss\n",
    "for _ in range(1):\n",
    "    for c, (y,g,w) in tqdm(enumerate(tr_dataloader)):\n",
    "        g = g.type(torch.float32)\n",
    "        w = w.type(torch.float32)\n",
    "        preds = model(g)\n",
    "        if preds[0].isnan():\n",
    "            print('----------')\n",
    "            print('y',y)\n",
    "            print('g',g)\n",
    "            print('w',w)\n",
    "            print('p', preds)\n",
    "            print('stop')\n",
    "            break\n",
    "        y = y.type(torch.float32)\n",
    "        preds= preds.squeeze()\n",
    "        predslist.append(preds.detach().numpy())\n",
    "        loss = loss_func(preds,y.squeeze())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "        if c > 1000:\n",
    "            print(c)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  1.,  3.,  1.,  2.,  4., 11., 14., 17., 19., 22., 32.,\n",
       "        19., 30., 35., 45., 50., 44., 62., 46., 46., 46., 49., 38., 46.,\n",
       "        37., 41., 32., 38., 38., 19., 20., 17., 18.,  9.,  9.,  8.,  2.,\n",
       "        10.,  7.,  3.,  1.,  2.,  1.,  3.,  1.,  0.,  1.,  2.]),\n",
       " array([0.4888756 , 0.51157963, 0.53428364, 0.5569877 , 0.57969171,\n",
       "        0.60239577, 0.62509978, 0.64780384, 0.67050785, 0.69321191,\n",
       "        0.71591592, 0.73861992, 0.76132399, 0.78402799, 0.80673206,\n",
       "        0.82943606, 0.85214013, 0.87484413, 0.8975482 , 0.9202522 ,\n",
       "        0.94295621, 0.96566027, 0.98836428, 1.01106834, 1.03377235,\n",
       "        1.05647635, 1.07918048, 1.10188448, 1.12458849, 1.14729249,\n",
       "        1.1699965 , 1.19270062, 1.21540463, 1.23810863, 1.26081264,\n",
       "        1.28351676, 1.30622077, 1.32892478, 1.35162878, 1.3743329 ,\n",
       "        1.39703691, 1.41974092, 1.44244492, 1.46514893, 1.48785305,\n",
       "        1.51055706, 1.53326106, 1.55596507, 1.57866919, 1.6013732 ,\n",
       "        1.6240772 ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(losses,bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = []\n",
    "\n",
    "for _ in range(5):\n",
    "    yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "    avg_yield = np.mean(yield_set['scaled_yield'])\n",
    "    model = MLP(input_size=100, hidden_size=64, output_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    envs = list(yield_set['Env'])\n",
    "    hybrids = list(yield_set['Hybrid'])\n",
    "    y = np.array(list(yield_set['scaled_yield']))\n",
    "    snp_idx = []\n",
    "    for i in hybrids:\n",
    "        snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "    g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "    w = []\n",
    "    for i in envs:\n",
    "        x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "        w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "    w=np.array(w)\n",
    "\n",
    "\n",
    "    g = torch.tensor(g.reshape(yield_set.shape[0],100)).type(torch.float32)\n",
    "    preds = model(g)\n",
    "    d = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "    samps.append(d.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.tensor(g.reshape(yield_set.shape[0],100)).type(torch.float32)\n",
    "preds = model(g)\n",
    "F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just G\n",
    "\n",
    "yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "loss_runs = []\n",
    "for _ in range(10):\n",
    "    losses = []\n",
    "    yield_set = gem.Y.Tr.sample(1000)\n",
    "    avg_yield = np.mean(yield_set['scaled_yield'])\n",
    "    model = MLP(input_size=100, hidden_size=200, output_size=1)\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.05)\n",
    "    for _ in tqdm(range(20)):\n",
    "        samples = yield_set.sample(bs)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(g.reshape(5,100)).type(torch.float32))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "#plt.axhline(y=np.array(avg_yield), color='red', linestyle='-')\n",
    "plt.title('MLP SNP data only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = []\n",
    "for _ in range(1000):\n",
    "    model = MLP(input_size=100, hidden_size=200, output_size=1)\n",
    "    samples = yield_set.sample(5)\n",
    "    preds = model(torch.tensor(g.reshape(5,100)).type(torch.float32))\n",
    "    for x in [x.detach().numpy() for x in preds]: raw_preds.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(yield_set['scaled_yield'])) / yield_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([x[0] for x in raw_preds],bins=20)\n",
    "plt.title('untrain model predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.Y.plot_yields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "#plt.axhline(y=np.array(avg_yield), color='red', linestyle='-')\n",
    "plt.title('MLP SNP data only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.reshape(5,365*16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just W\n",
    "\n",
    "#yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.fc2(x)\n",
    "       # x = torch.clip(x,max=1)\n",
    "        return x\n",
    "\n",
    "loss_runs = []\n",
    "for _ in range(4):\n",
    "    losses = []\n",
    "    model = MLP(input_size=5840, hidden_size=64, output_size=1)\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.1)\n",
    "    for _ in tqdm(range(250)):\n",
    "        samples = yield_set.sample(bs)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "        w = w.reshape(5,365*16)\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(w).type(torch.float32))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP(input_size=100, hidden_size=64, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [] \n",
    "for i in envs:\n",
    "    x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "    w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "w=np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(bs,x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "cnn = OneDCNN()\n",
    "wt = torch.tensor(w).type(torch.float32)\n",
    "wt = wt.reshape(5,16,365)\n",
    "cnn(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_runs = []\n",
    "for _ in range(4):\n",
    "    losses = []\n",
    "    model = OneDCNN()\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.01, weight_decay=.001)\n",
    "    for _ in tqdm(range(250)):\n",
    "        samples = yield_set.sample(5)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "        wt = torch.tensor(w).type(torch.float32)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(wt))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.title('Weather 1D Cnn + decay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(bs,x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GxE(nn.Module):\n",
    "    def __init__(self,mlp, cnn, batch_size):\n",
    "        super(GxE, self).__init__()\n",
    "        self.MLP = mlp\n",
    "        self.OneDCNN = cnn\n",
    "        self.fc1 = nn.Linear(200,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        g,w = x\n",
    "        g = g.reshape(5,100)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "        \n",
    "        \n",
    "        g = self.MLP(g)\n",
    "        w = self.OneDCNN(w)\n",
    "        \n",
    "        x= torch.concat((g,w),axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = GxE(MLP(100,50, 50), OneDCNN(), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = torch.tensor(g)\n",
    "g = g.reshape(5,100).type(torch.float32)\n",
    "\n",
    "w=np.array(w)\n",
    "wt = torch.tensor(w).type(torch.float32)\n",
    "wt = wt.reshape(5,16,365)\n",
    "\n",
    "model((g,wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_set = gem.Y.Tr.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_runs = []\n",
    "ys=[]\n",
    "ps= []\n",
    "for LR in [.01,.05,.1,.2]:\n",
    "    losses = []\n",
    "    model = GxE(MLP(100,200,100), OneDCNN())\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=LR, weight_decay=.005)\n",
    "    for _ in tqdm(range(2500)):\n",
    "        samples = gem.Y.Tr.sample(5)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        \n",
    "        \n",
    "        g = torch.tensor(g)\n",
    "        g = g.reshape(5,100).type(torch.float32)\n",
    "\n",
    "        w=np.array(w)\n",
    "        wt = torch.tensor(w).type(torch.float32)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "\n",
    "        preds = model((g,wt))\n",
    "        #train loop\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "        [ps.append(x) for x in preds.detach().numpy()]\n",
    "        [ys.append(x) for x in y]\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.polyfit(ys, ps, 1)\n",
    "plt.scatter(ys,ps)\n",
    "plt.plot(ys, a * ys + b, '-r')\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ps[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot((x))\n",
    "plt.title('GxE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(running_average(loss_runs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_average(loss_runs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_average(loss_runs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(o.type(torch.float32).to(device) for o in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = tr_dataloader, te_dataloader\n",
    "type(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train,self.valid = dls[:2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds), **kwargs) for ds in dd.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_device = 'cpu'\n",
    "dls = DataLoaders(tr_dataloader, te_dataloader)\n",
    "dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [CompletionCB()]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learn.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def callback_ctx(cbmeth, nm):\n",
    "    try:\n",
    "        cbmeth(f'before_{nm}')\n",
    "        yield\n",
    "        cbmeth(f'after_{nm}')\n",
    "    except globals()[f'Cancel{nm.title()}Exception']: pass\n",
    "    finally: cbmeth(f'cleanup_{nm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        self.cb_ctx = partial(callback_ctx, self.callback)\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.cb_ctx('epoch'):\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                with self.cb_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()\n",
    "    \n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n",
    "            with self.cb_ctx('fit'):\n",
    "                for self.epoch in self.epochs:\n",
    "                    if train: self.one_epoch(True)\n",
    "                    if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x =  x.reshape(x.shape[0],16,365)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0],x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GxE(nn.Module):\n",
    "    def __init__(self,mlp, cnn):\n",
    "        super(GxE, self).__init__()\n",
    "        self.MLP = mlp\n",
    "        self.OneDCNN = cnn\n",
    "        self.fc1 = nn.Linear(200,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        g,w = x\n",
    "#        g = g.reshape(batch_size,100)\n",
    "        \n",
    "        \n",
    "        g = self.MLP(g)\n",
    "        w = self.OneDCNN(w)\n",
    "        \n",
    "        x= torch.concat((g,w),axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = GxE(MLP(100,100, 100), OneDCNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(MeanSquaredError())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "        #self.smooth_losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            #self.smooth_losses.append((sum(self.smooth_losses) + learn.loss.item())/(len(self.smooth_losses)+1))\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        y,_,_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds.squeeze()), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainCB(Callback):\n",
    "    def predict(self, learn):\n",
    "        learn.preds = learn.model((learn.batch[1],learn.batch[2]))\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds.squeeze(), learn.batch[0])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(te_dataloader, tr_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "metrics = MetricsCB(MeanSquaredError())\n",
    "cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "#cbs = [TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, F.mse_loss, lr=.25, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input sequences\n",
    "x1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Define the transformer model\n",
    "model = nn.Transformer(d_model=512, nhead=8)\n",
    "\n",
    "# Pass the input sequences through the model\n",
    "output = model(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionTransformer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # Define the attention layers\n",
    "        self.self_attention1 = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "        self.self_attention2 = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "        self.cross_attention = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "\n",
    "        # Define the feedforward layers\n",
    "        self.feedforward1 = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        self.feedforward2 = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Apply self attention to the first input sequence\n",
    "        x1 = self.self_attention1(x1, x1, x1)[0]\n",
    "        x1 = self.feedforward1(x1)\n",
    "\n",
    "        # Apply self attention to the second input sequence\n",
    "        x2 = self.self_attention2(x2, x2, x2)[0]\n",
    "        x2 = self.feedforward2(x2)\n",
    "\n",
    "        # Apply cross attention between the two input sequences\n",
    "        x = self.cross_attention(x1, x2, x2)[0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sequences\n",
    "x1 = torch.tensor([[1, 2, 3, 3], [4, 5, 6, 7]])\n",
    "x2 = torch.tensor([[7, 8, 9, 4], [10, 11, 12, 13]])\n",
    "\n",
    "# Define the model\n",
    "model = CrossAttentionTransformer(input_size=4, hidden_size=64, num_attention_heads=4)\n",
    "\n",
    "# Pass the input sequences through the model\n",
    "output = model(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input sequences\n",
    "query = torch.tensor([[1, 2, 3, 3], [4, 5, 6, 7]]).type(torch.float32)\n",
    "key = torch.tensor([[7, 8, 9,4 ], [10, 11, 12,13]]).type(torch.float32)\n",
    "value = torch.tensor([[13, 14, 15, 12], [16, 17, 18, 13]]).type(torch.float32)\n",
    "\n",
    "# Define the attention module\n",
    "attention = nn.MultiheadAttention(embed_dim=4, num_heads=2)\n",
    "\n",
    "# Compute the attention weights and output\n",
    "output, weights = attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

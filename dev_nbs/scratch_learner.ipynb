{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp GEMlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMdataset\n",
    "\n",
    "> This package will hold the GxExM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "#| hide\n",
    "from hybridpredictmaize22.GEMdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#|export\n",
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from warnings import warn\n",
    "\n",
    "from torch import tensor,nn,optim\n",
    "from torch.utils.data import DataLoader,default_collate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from fastprogress import progress_bar,master_bar\n",
    "from torcheval.metrics import MeanSquaredError,Mean, R2Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def remove_leapdays(weather):\n",
    "    \"\"\" just a hotfix \"\"\"\n",
    "    to_remove = []\n",
    "    for i in list(set(weather_data['Env'])):\n",
    "        if (sum(weather_data['Env'] == i)) == 366:\n",
    "            #get indexes\n",
    "            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))\n",
    "    return weather_data.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = {\n",
    "'test_split':2019,\n",
    "'path_snps':Path('data/snpCompress/'),\n",
    "'data_path':Path('data/'),\n",
    "'path_train_weatherTable':Path('data/Training_Data/4_Training_Weather_Data_2014_2021.csv'),\n",
    "'path_train_yieldTable':Path('data/Training_Data/1_Training_Trait_Data_2014_2021.csv'),\n",
    "'snp_compression':'PCS_50',\n",
    "'batch_size':64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('data/snpCompress/')\n",
    "data_path = Path('data/')\n",
    "path_train_weatherTable =data_path/'Training_Data/4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'Training_Data/1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_50'\n",
    "batch_size = 1\n",
    "\n",
    "def setup_data():\n",
    "    \n",
    "    snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "    yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "    weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "    \n",
    "    yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "    #yield_data = yield_data.reset_index()\n",
    "    weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "    #removes yield data where no weather data\n",
    "    setYield = set(yield_data['Env'])\n",
    "    setWeather = set(weather_data['Env'])\n",
    "    only_yield = setYield - setWeather\n",
    "    only_weather = setWeather - setYield\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "    #removes yield data where no genotype data\n",
    "    setSNP = set(snp_data[0])\n",
    "    setYield = set(yield_data['Hybrid'])\n",
    "    only_yield = setYield - setSNP\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "    weather_data = remove_leapdays(weather_data)\n",
    "    #weather_data = weather_data.reset_index()\n",
    "    #yield_data=yield_data.sample(frac=1)\n",
    "    yield_data = yield_data.reset_index()\n",
    "    \n",
    "    #Create a GEM dataset\n",
    "    gem = GEM(test_split)\n",
    "    gem.Y = YT(yield_data, test_split)\n",
    "    gem.W = WT(weather_data, test_split)\n",
    "    gem.SNP = snp_data\n",
    "    \n",
    "    tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "    te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "    \n",
    "    tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return tr_dataloader, te_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop('column_name', axis=1, inplace=True)\n",
    "\n",
    "yield_data.drop('index',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mgem\u001b[49m\u001b[38;5;241m.\u001b[39mY\u001b[38;5;241m.\u001b[39mTr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_yield\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gem' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(gem.Y.Tr['scaled_yield'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(gem.Y.Tr['scaled_yield']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "#Create a GEM dataset\n",
    "gem = GEM(test_split)\n",
    "\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dls = (tr_dataloader, te_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_g, input_w, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = (nn.Linear(input_g, hidden_size))\n",
    "        self.fc2 = (nn.Linear(input_w, hidden_size))\n",
    "        self.fc22 = (nn.Linear(hidden_size,hidden_size))\n",
    "        self.fc3 = (nn.Linear(hidden_size*2,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        g,w = x\n",
    "        \n",
    "        g = self.fc1(g)\n",
    "        g = torch.relu(g)\n",
    "\n",
    "        w = torch.reshape(batch_size)        \n",
    "        #w = nn.Flatten(w)\n",
    "        w = self.fc2(w)\n",
    "        w = torch.relu(w)\n",
    "        w = self.fc22(w)\n",
    "        \n",
    "        h = torch.concat((g.squeeze(),w))\n",
    "        h = self.fc3(h)\n",
    "        \n",
    "        h = torch.clip(h,max=2)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "predslist = []\n",
    "model = MLP(100,5840,5000,1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=.01)\n",
    "loss_func = F.mse_loss\n",
    "for _ in range(1):\n",
    "    for c, (y,g,w) in tqdm(enumerate(tr_dataloader)):\n",
    "        g = g.type(torch.float32)\n",
    "        w = w.type(torch.float32)\n",
    "        preds = model([g,w])\n",
    "        if preds[0].isnan():\n",
    "            print('----------')\n",
    "            print('y',y)\n",
    "            print('g',g)\n",
    "            print('w',w)\n",
    "            print('p', preds)\n",
    "            print('stop')\n",
    "            break\n",
    "        y = y.type(torch.float32)\n",
    "        preds= preds.squeeze()\n",
    "        predslist.append(preds.detach().numpy())\n",
    "        loss = loss_func(preds,y.squeeze())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "        if c > 1000:\n",
    "            print(c)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samps = []\n",
    "\n",
    "for _ in range(5):\n",
    "    yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "    avg_yield = np.mean(yield_set['scaled_yield'])\n",
    "    model = MLP(input_size=100, hidden_size=64, output_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    envs = list(yield_set['Env'])\n",
    "    hybrids = list(yield_set['Hybrid'])\n",
    "    y = np.array(list(yield_set['scaled_yield']))\n",
    "    snp_idx = []\n",
    "    for i in hybrids:\n",
    "        snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "    g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "    w = []\n",
    "    for i in envs:\n",
    "        x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "        w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "    w=np.array(w)\n",
    "\n",
    "\n",
    "    g = torch.tensor(g.reshape(yield_set.shape[0],100)).type(torch.float32)\n",
    "    preds = model(g)\n",
    "    d = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "    samps.append(d.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.tensor(g.reshape(yield_set.shape[0],100)).type(torch.float32)\n",
    "preds = model(g)\n",
    "F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just G\n",
    "\n",
    "yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "loss_runs = []\n",
    "for _ in range(10):\n",
    "    losses = []\n",
    "    yield_set = gem.Y.Tr.sample(1000)\n",
    "    avg_yield = np.mean(yield_set['scaled_yield'])\n",
    "    model = MLP(input_size=100, hidden_size=200, output_size=1)\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.05)\n",
    "    for _ in tqdm(range(20)):\n",
    "        samples = yield_set.sample(bs)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(g.reshape(5,100)).type(torch.float32))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "#plt.axhline(y=np.array(avg_yield), color='red', linestyle='-')\n",
    "plt.title('MLP SNP data only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = []\n",
    "for _ in range(1000):\n",
    "    model = MLP(input_size=100, hidden_size=200, output_size=1)\n",
    "    samples = yield_set.sample(5)\n",
    "    preds = model(torch.tensor(g.reshape(5,100)).type(torch.float32))\n",
    "    for x in [x.detach().numpy() for x in preds]: raw_preds.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(abs(yield_set['scaled_yield'])) / yield_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([x[0] for x in raw_preds],bins=20)\n",
    "plt.title('untrain model predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.Y.plot_yields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "#plt.axhline(y=np.array(avg_yield), color='red', linestyle='-')\n",
    "plt.title('MLP SNP data only')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.reshape(5,365*16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just W\n",
    "\n",
    "#yield_set = gem.Y.Tr.iloc[:1000,:]\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.fc2(x)\n",
    "       # x = torch.clip(x,max=1)\n",
    "        return x\n",
    "\n",
    "loss_runs = []\n",
    "for _ in range(4):\n",
    "    losses = []\n",
    "    model = MLP(input_size=5840, hidden_size=64, output_size=1)\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.1)\n",
    "    for _ in tqdm(range(250)):\n",
    "        samples = yield_set.sample(bs)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "        w = w.reshape(5,365*16)\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(w).type(torch.float32))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP(input_size=100, hidden_size=64, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [] \n",
    "for i in envs:\n",
    "    x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "    w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "w=np.array(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(bs,x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "cnn = OneDCNN()\n",
    "wt = torch.tensor(w).type(torch.float32)\n",
    "wt = wt.reshape(5,16,365)\n",
    "cnn(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_runs = []\n",
    "for _ in range(4):\n",
    "    losses = []\n",
    "    model = OneDCNN()\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=.01, weight_decay=.001)\n",
    "    for _ in tqdm(range(250)):\n",
    "        samples = yield_set.sample(5)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        w=np.array(w)\n",
    "        wt = torch.tensor(w).type(torch.float32)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "        #train loop\n",
    "        preds  = model(torch.tensor(wt))\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot(running_average(x))\n",
    "plt.title('Weather 1D Cnn + decay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(bs,x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GxE(nn.Module):\n",
    "    def __init__(self,mlp, cnn, batch_size):\n",
    "        super(GxE, self).__init__()\n",
    "        self.MLP = mlp\n",
    "        self.OneDCNN = cnn\n",
    "        self.fc1 = nn.Linear(200,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        g,w = x\n",
    "        g = g.reshape(5,100)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "        \n",
    "        \n",
    "        g = self.MLP(g)\n",
    "        w = self.OneDCNN(w)\n",
    "        \n",
    "        x= torch.concat((g,w),axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = GxE(MLP(100,50, 50), OneDCNN(), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = torch.tensor(g)\n",
    "g = g.reshape(5,100).type(torch.float32)\n",
    "\n",
    "w=np.array(w)\n",
    "wt = torch.tensor(w).type(torch.float32)\n",
    "wt = wt.reshape(5,16,365)\n",
    "\n",
    "model((g,wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_set = gem.Y.Tr.iloc[:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_runs = []\n",
    "ys=[]\n",
    "ps= []\n",
    "for LR in [.01,.05,.1,.2]:\n",
    "    losses = []\n",
    "    model = GxE(MLP(100,200,100), OneDCNN())\n",
    "    opt = torch.optim.SGD(model.parameters(),lr=LR, weight_decay=.005)\n",
    "    for _ in tqdm(range(2500)):\n",
    "        samples = gem.Y.Tr.sample(5)\n",
    "        envs = list(samples['Env'])\n",
    "        hybrids = list(samples['Hybrid'])\n",
    "        y = np.array(list(samples['scaled_yield']))\n",
    "        snp_idx = []\n",
    "        for i in hybrids:\n",
    "            snp_idx.append(np.where(gem.SNP[0]==i)[0][0])\n",
    "        g = gem.SNP[1][:,snp_idx]\n",
    "\n",
    "        w = []\n",
    "        for i in envs:\n",
    "            x = gem.W.Tr[gem.W.Tr['Env'] == i]\n",
    "            w.append(np.array(x.iloc[:365,:].select_dtypes('float')))\n",
    "        \n",
    "        \n",
    "        g = torch.tensor(g)\n",
    "        g = g.reshape(5,100).type(torch.float32)\n",
    "\n",
    "        w=np.array(w)\n",
    "        wt = torch.tensor(w).type(torch.float32)\n",
    "        wt = wt.reshape(5,16,365)\n",
    "\n",
    "        preds = model((g,wt))\n",
    "        #train loop\n",
    "        loss = F.mse_loss(preds,torch.tensor(y).unsqueeze(axis=1).type(torch.float32))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        losses.append(loss.detach().numpy())\n",
    "        [ps.append(x) for x in preds.detach().numpy()]\n",
    "        [ys.append(x) for x in y]\n",
    "    loss_runs.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.polyfit(ys, ps, 1)\n",
    "plt.scatter(ys,ps)\n",
    "plt.plot(ys, a * ys + b, '-r')\n",
    "plt.xlim(-2,2)\n",
    "plt.ylim(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ps[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in loss_runs:\n",
    "    plt.plot((x))\n",
    "plt.title('GxE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(running_average(loss_runs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_average(loss_runs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(running_average(loss_runs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(o.type(torch.float32).to(device) for o in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = tr_dataloader, te_dataloader\n",
    "type(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, *dls): self.train,self.valid = dls[:2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        return cls(*[DataLoader(ds, batch_size, collate_fn=collate_dict(ds), **kwargs) for ds in dd.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_device = 'cpu'\n",
    "dls = DataLoaders(tr_dataloader, te_dataloader)\n",
    "dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.preds = self.model(self.batch[0])\n",
    "        self.loss = self.loss_func(self.preds, self.batch[1])\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [CompletionCB()]\n",
    "learn = Learner(model, dls, F.cross_entropy, lr=0.2, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(learn.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def callback_ctx(cbmeth, nm):\n",
    "    try:\n",
    "        cbmeth(f'before_{nm}')\n",
    "        yield\n",
    "        cbmeth(f'after_{nm}')\n",
    "    except globals()[f'Cancel{nm.title()}Exception']: pass\n",
    "    finally: cbmeth(f'cleanup_{nm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        self.cb_ctx = partial(callback_ctx, self.callback)\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.cb_ctx('epoch'):\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                with self.cb_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()\n",
    "    \n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n",
    "            with self.cb_ctx('fit'):\n",
    "                for self.epoch in self.epochs:\n",
    "                    if train: self.one_epoch(True)\n",
    "                    if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "class OneDCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneDCNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=16, out_channels=100, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=200, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=200, out_channels=400, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(146000, 100)\n",
    "        self.fc2 = nn.Linear(100, 100)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the layers\n",
    "        x =  x.reshape(x.shape[0],16,365)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0],x.shape[1]*x.shape[2])\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GxE(nn.Module):\n",
    "    def __init__(self,mlp, cnn):\n",
    "        super(GxE, self).__init__()\n",
    "        self.MLP = mlp\n",
    "        self.OneDCNN = cnn\n",
    "        self.fc1 = nn.Linear(200,100)\n",
    "        self.fc2 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        g,w = x\n",
    "#        g = g.reshape(batch_size,100)\n",
    "        \n",
    "        \n",
    "        g = self.MLP(g)\n",
    "        w = self.OneDCNN(w)\n",
    "        \n",
    "        x= torch.concat((g,w),axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = GxE(MLP(100,100, 100), OneDCNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(MeanSquaredError())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "        #self.smooth_losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            #self.smooth_losses.append((sum(self.smooth_losses) + learn.loss.item())/(len(self.smooth_losses)+1))\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        y,_,_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds.squeeze()), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TrainCB(Callback):\n",
    "    def predict(self, learn):\n",
    "        learn.preds = learn.model((learn.batch[1],learn.batch[2]))\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds.squeeze(), learn.batch[0])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(te_dataloader, tr_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "metrics = MetricsCB(MeanSquaredError())\n",
    "cbs = [TrainCB(), DeviceCB(), metrics, ProgressCB(plot=True)]\n",
    "#cbs = [TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, F.mse_loss, lr=.25, cbs=cbs)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.metrics.loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input sequences\n",
    "x1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Define the transformer model\n",
    "model = nn.Transformer(d_model=512, nhead=8)\n",
    "\n",
    "# Pass the input sequences through the model\n",
    "output = model(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionTransformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_attention_heads):\n",
    "        super(CrossAttentionTransformer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # Define the attention layers\n",
    "        self.self_attention1 = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "        self.self_attention2 = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "        self.cross_attention = nn.MultiheadAttention(self.input_size, self.num_attention_heads)\n",
    "\n",
    "        # Define the feedforward layers\n",
    "        self.feedforward1 = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        self.feedforward2 = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Apply self attention to the first input sequence\n",
    "        x1 = self.self_attention1(x1, x1, x1)[0]\n",
    "        x1 = self.feedforward1(x1)\n",
    "\n",
    "        # Apply self attention to the second input sequence\n",
    "        x2 = self.self_attention2(x2, x2, x2)[0]\n",
    "        x2 = self.feedforward2(x2)\n",
    "\n",
    "        # Apply cross attention between the two input sequences\n",
    "        x = self.cross_attention(x1, x2, x2)[0]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sequences\n",
    "x1 = torch.tensor([[1, 2, 3, 3], [4, 5, 6, 7]])\n",
    "x2 = torch.tensor([[7, 8, 9, 4], [10, 11, 12, 13]])\n",
    "\n",
    "# Define the model\n",
    "model = CrossAttentionTransformer(input_size=4, hidden_size=64, num_attention_heads=4)\n",
    "\n",
    "# Pass the input sequences through the model\n",
    "output = model(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input sequences\n",
    "query = torch.tensor([[1, 2, 3, 3], [4, 5, 6, 7]]).type(torch.float32)\n",
    "key = torch.tensor([[7, 8, 9,4 ], [10, 11, 12,13]]).type(torch.float32)\n",
    "value = torch.tensor([[13, 14, 15, 12], [16, 17, 18, 13]]).type(torch.float32)\n",
    "\n",
    "# Define the attention module\n",
    "attention = nn.MultiheadAttention(embed_dim=4, num_heads=2)\n",
    "\n",
    "# Compute the attention weights and output\n",
    "output, weights = attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e32a0-c04b-4025-9dc0-2cbeb2a2d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hybridpredictmaize22.hybridpredictmaize22.snpCompression import *\n",
    "from hybridpredictmaize22.GEMlearn import *\n",
    "from hybridpredictmaize22.GEMdataset import *\n",
    "from hybridpredictmaize22.snpCompression import *\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from warnings import warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4602a-990d-411a-89d2-99cfa0ce7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "#| export\n",
    "class newGemDataset():\n",
    "    \"\"\"\n",
    "    Pytorch Dataset which can be used with dataloaders for simple batching during training loops\n",
    "    \"\"\"\n",
    "    def __init__(self,W,Y,G, def_device='cpu'):\n",
    "        self.W = W\n",
    "        self.SNP = G\n",
    "        self.Y = Y\n",
    "        self.device = def_device\n",
    "        \n",
    "    def __len__(self): return self.Y[0].shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "      y = self.Y[0][idx]\n",
    "      e = self.Y[1][idx]\n",
    "      h = self.Y[2][idx]\n",
    "      d = self.Y[3][idx]\n",
    "\n",
    "      #weather\n",
    "      w = self.W[1][np.where(self.W[0] == e)[0][0]]\n",
    "\n",
    "      #snp\n",
    "      g = snp_data[1][:,np.where(snp_data[0] == h)[0][0]]\n",
    "      return y,g,w\n",
    "\n",
    "\n",
    "#| export\n",
    "class ST():\n",
    "    \"\"\"\n",
    "    A class which will hold the secondary trait data for the entire dataset for pre-training purposes\n",
    "    \n",
    "    init\n",
    "        yield_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, yield_data, testYear):\n",
    "\n",
    "        self.Te = yield_data.iloc[([str(testYear) in x for x in yield_data['Env']]),:].reset_index()\n",
    "        self.Tr = yield_data.iloc[([str(testYear) not in x for x in yield_data['Env']]),:].reset_index()\n",
    "\n",
    "        self.secondary_traits = [\n",
    "               # 'Stand_Count_plants',\n",
    "               # 'Pollen_DAP_days',\n",
    "               # 'Silk_DAP_days',\n",
    "               # 'Plant_Height_cm',\n",
    "               # 'Ear_Height_cm',\n",
    "                #'Root_Lodging_plants',\n",
    "                #'Stalk_Lodging_plants',\n",
    "               # 'Twt_kg_m3',\n",
    "                'Yield_Mg_ha',\n",
    "                #'Date_Harvested'\n",
    "                ]\n",
    "        \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_arrays(self.Tr)\n",
    "        self.make_arrays(self.Te, False)\n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(np.array(self.Tr[self.secondary_traits]))\n",
    "        self.scaler = ss\n",
    "\n",
    "    def scale_data(self,df):\n",
    "        scaled_secondary = self.scaler.transform(np.array(df[self.secondary_traits]))\n",
    "        for c,i in enumerate(self.secondary_traits):\n",
    "            #print(i)\n",
    "            df[i] = scaled_secondary[:,c]\n",
    "    \n",
    "    def plot_yields(self):\n",
    "        for i in self.secondary_traits:\n",
    "            plt.hist(self.Tr[i],density=True, label='Train',alpha=.5,bins=50)\n",
    "            plt.hist(self.Te[i],density=True, label='Test',alpha=.5,bins=50)\n",
    "            plt.legend()\n",
    "            plt.title(i)\n",
    "            plt.show()\n",
    "\n",
    "    def make_arrays(self,df,train=True):\n",
    "      df = np.array(df[self.secondary_traits]), np.array(df['Env']) , np.array(df['Hybrid']), np.array(df['Date_Planted'])\n",
    "      if train:\n",
    "        self.Tr = df\n",
    "      else:\n",
    "        self.Te= df\n",
    "\n",
    "#| export\n",
    "class newWT():\n",
    "    \"\"\"\n",
    "    A class which will hold the weather data for the entire dataset for training purposes\n",
    "    \n",
    "    init\n",
    "        weather_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, weather_data, testYear):\n",
    "        \n",
    "        self.Te = weather_data.iloc[([str(testYear) in x for x in weather_data['Year']]),:].reset_index()\n",
    "        self.Tr = weather_data.iloc[([str(testYear) not in x for x in weather_data['Year']]),:].reset_index()\n",
    "            \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_array(self.Tr)\n",
    "        self.make_array(self.Te,False)\n",
    "            \n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(self.Tr.select_dtypes('float'))\n",
    "        self.scaler = ss\n",
    "            \n",
    "    def scale_data(self, df):\n",
    "        fd = df.select_dtypes('float')\n",
    "        fs = self.scaler.transform(fd)\n",
    "        df[fd.columns] = fs\n",
    "\n",
    "    def make_array(self, df,train = True):\n",
    "      for c,i in enumerate(set(df['Env'])):\n",
    "        env_weather = np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "        #print(env_weather.shape)\n",
    "        if c == 0:\n",
    "          env_order = list([i])\n",
    "          weather_array =   np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "          weather_array = np.expand_dims(weather_array,axis=0)\n",
    "        else:\n",
    "          weather_array = np.vstack((weather_array, env_weather[None,:,:]))\n",
    "          env_order.append(i)\n",
    "\n",
    "        if train:\n",
    "          self.Tr = (np.array(env_order), np.array(weather_array))\n",
    "        else:\n",
    "          self.Te = (np.array(env_order), np.array(weather_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cfed0-0f7b-43cd-865b-01f32194f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "test_year=2019\n",
    "\n",
    "path_snps = Path('./data/snpCompress/')\n",
    "data_path = Path('./data/Training_Data/')\n",
    "path_train_weatherTable =data_path/'4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_100'\n",
    "batch_size = 64\n",
    "\n",
    "snp_data = collect_snps(Path('./data/snpCompress/PCS_50/')) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "to_remove = []\n",
    "for i in set(weather_data['Env']):\n",
    "    for x in (weather_data.loc[weather_data['Env'] == i].index[300:]):\n",
    "        to_remove.append(x)\n",
    "        \n",
    "weather_data_clean = weather_data.drop(index=to_remove)\n",
    "\n",
    "\n",
    "\n",
    "#weather_data = remove_leapdays(weather_data)\n",
    "weather_data_clean = weather_data_clean.reset_index()\n",
    "yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "W = newWT(weather_data_clean,test_year)\n",
    "Y=ST(yield_data,test_year)\n",
    "\n",
    "tr_gem = newGemDataset(W=W.Tr,\n",
    "                    Y = Y.Tr,\n",
    "                    G = snp_data)\n",
    "te_gem = newGemDataset(W=W.Te,\n",
    "                    Y= Y.Te,\n",
    "                    G = snp_data)\n",
    "\n",
    "# tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP, def_device='cpu')\n",
    "# te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP, def_device ='cpu')\n",
    "\n",
    "tr_dataloader = DataLoader(tr_gem, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "te_dataloader = DataLoader(te_gem, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4b08e-40b4-433f-88d0-ecf64310b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 16)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.Tr[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e5bbb-a65e-4860-b3d4-8ada30645c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet2(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1, groups=128, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EfficientNet3(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet3, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1,  bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class NNEnsemble3(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list, alpha=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.lin1 = torch.nn.Linear(hidden_list[0], hidden_list[1])\n",
    "    self.lin2 = torch.nn.Linear(hidden_list[1], hidden_list[2])\n",
    "    self.out = torch.nn.Linear(hidden_list[2], 1)\n",
    "\n",
    "    \n",
    "    if alpha != None:\n",
    "        print('X')\n",
    "        for c,l in enumerate(self.layers):\n",
    "            print(l)\n",
    "            torch.nn.init.xavier_normal_(l.weight,gain=alpha)\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    " #   print(g.shape, w.shape)\n",
    "\n",
    "    if w.dim() == 3:\n",
    "      w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    if g.dim() == 3:\n",
    "      g = g.view(g.shape[0], g.shape[1] * g.shape[2])\n",
    "\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = torch.nn.functional.relu(x) \n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09735cb4-6158-4146-ac08-28ebdad867c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet2(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1, groups=128, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EfficientNet3(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet3, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1,  bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class NNEnsemble3(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list, alpha=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.lin1 = torch.nn.Linear(hidden_list[0], hidden_list[1])\n",
    "    self.lin2 = torch.nn.Linear(hidden_list[1], hidden_list[2])\n",
    "    self.out = torch.nn.Linear(hidden_list[2], 1)\n",
    "\n",
    "    \n",
    "    if alpha != None:\n",
    "        print('X')\n",
    "        for c,l in enumerate(self.layers):\n",
    "            print(l)\n",
    "            torch.nn.init.xavier_normal_(l.weight,gain=alpha)\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    " #   print(g.shape, w.shape)\n",
    "\n",
    "    if w.dim() == 3:\n",
    "      w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    if g.dim() == 3:\n",
    "      g = g.view(g.shape[0], g.shape[1] * g.shape[2])\n",
    "\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = torch.nn.functional.relu(x) \n",
    "    return self.out(x)\n",
    "\n",
    "def moving_average(arr, window_size):\n",
    "    \"\"\"Calculate the moving average of an array.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (np.ndarray): Input array with shape (n_samples,).\n",
    "    window_size (int): Size of the moving window.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Moving average of the array, with shape (n_samples - window_size + 1,).\n",
    "    \"\"\"\n",
    "    # Initialize a NumPy array to store the moving averages\n",
    "    ma = np.zeros(len(arr) - window_size + 1)\n",
    "    \n",
    "    # Calculate the moving average\n",
    "    for i in range(len(ma)):\n",
    "        ma[i] = np.mean(arr[i:i+window_size])\n",
    "        \n",
    "    return ma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414027c-7598-40a0-9000-c740f5ef818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▋                                                                                 | 1/50 [00:01<01:01,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7951, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:59<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "gm = EfficientNet3(num_classes=1500)\n",
    "wm = EfficientNet3(300, num_classes=1500)\n",
    "                    \n",
    "\n",
    "model = NNEnsemble3([2000,1000,1000], [gm,wm],alpha=2)\n",
    "# plt.hist(detach_list(model((g,w))))\n",
    "\n",
    "opt = optim.Adamax(model.parameters(), lr=.0005, weight_decay=.0001)\n",
    "#scheduler =  optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "\n",
    "loss_func = torch.nn.functional.huber_loss\n",
    "\n",
    "tr_loss = [] \n",
    "te_loss = []\n",
    "te_MA = []\n",
    "predicts = []\n",
    "targets = []\n",
    "\n",
    "#for i in tqdm(range(len(tr_dataloader))):\n",
    "for i in tqdm(range(50)):\n",
    "  model.train()\n",
    "  #train loop\n",
    "  y,g,w = next(iter(tr_dataloader))\n",
    "  y =  y[:,-1]\n",
    "  y = y.type(torch.float32)\n",
    "  g = g.type(torch.float32)\n",
    "  w = w.type(torch.float32)\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y, preds)\n",
    "\n",
    "  loss.backward()\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "  #scheduler.step(loss)\n",
    "  tr_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "  #test loop\n",
    "  model.eval()\n",
    "  y,g,w = next(iter(te_dataloader))\n",
    "  y =  y[:,-1]\n",
    "  y = y.to(torch.float32)\n",
    "  g = g.to(torch.float32)\n",
    "  w = w.to(torch.float32)\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y,preds)\n",
    "\n",
    "  te_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    print(i, loss)\n",
    "\n",
    "  if i > 15:\n",
    "    te_MA.append(moving_average(te_loss[i-15:],14)[-1])\n",
    "\n",
    "    if i > 80:\n",
    "        if ((te_MA[-int(round((0.45 * len(te_loss)),1))] < te_loss[i])):\n",
    "            plt.plot(tr_loss)\n",
    "            plt.plot(te_loss)\n",
    "            plt.show()\n",
    "            print('')\n",
    "            print('early')\n",
    "            print(te_loss[-1])\n",
    "\n",
    "            plt.plot(tr_loss)\n",
    "            plt.plot(te_loss)\n",
    "            plt.ylim(0,1)\n",
    "            plt.show()\n",
    "\n",
    "            plt.ylim(0,.056)\n",
    "            plt.plot(tr_loss)\n",
    "            plt.plot(te_loss)\n",
    "            plt.show()\n",
    "\n",
    "            break\n",
    "\n",
    "  \n",
    "  \n",
    "# plt.plot(tr_loss)\n",
    "# plt.plot(te_loss)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b6704-45d5-4d98-bd89-3f21cf3f7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = []\n",
    "predicted = []\n",
    "for y,g,w in te_dataloader:\n",
    "    model.eval()\n",
    "    preds = model((g.type(torch.float32),w.type(torch.float32)))\n",
    "    \n",
    "    for i in preds:\n",
    "        predicted.append(i.detach().numpy())\n",
    "    for x in y:\n",
    "        ys.append(x.detach().numpy())\n",
    "        \n",
    "mypred = np.array(predicted).ravel()\n",
    "mytarget = np.array(ys)\n",
    "\n",
    "\n",
    "rmse = []\n",
    "for p,t in zip(mypred,mytarget):\n",
    "    if not np.isnan(t[-1]) or np.isnan(p):\n",
    "        target_scaled = (Y.scaler.inverse_transform(np.array(t).reshape(1,-1))[0][-1])\n",
    "        pred_scaled = (Y.scaler.inverse_transform(np.array(p).reshape(-1,1)))\n",
    "        rmse.append(np.sqrt((target_scaled-pred_scaled)**2))\n",
    "    else:\n",
    "        pass\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9be45-061e-4b08-82aa-be10d685bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypred = np.array(predicted).ravel()\n",
    "mytarget = np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66511b77-6fc0-4a0a-9fd6-e0c05a6a04b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5782666\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "rmse = []\n",
    "for p,t in zip(mypred,mytarget):\n",
    "    if not np.isnan(t[-1]) or np.isnan(p):\n",
    "        target_scaled = (Y.scaler.inverse_transform(np.array(t).reshape(1,-1))[0][-1])\n",
    "        pred_scaled = (Y.scaler.inverse_transform(np.array(p).reshape(-1,1)))\n",
    "        rmse.append(np.sqrt((target_scaled-pred_scaled)**2))\n",
    "    else:\n",
    "        pass\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0060ef-4cf4-4301-ab1b-f8a92369d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=[column_name], inplace=True)\n",
    "\n",
    "weather_data_clean.drop(columns=['Year'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087df67-2f68-47de-9a58-3771d1636bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721ad66-f52d-42ca-bb05-3ffc302b40fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Env</th>\n",
       "      <th>Date</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>PS</th>\n",
       "      <th>WS2M</th>\n",
       "      <th>ALLSKY_SFC_SW_DWN</th>\n",
       "      <th>RH2M</th>\n",
       "      <th>GWETTOP</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>GWETROOT</th>\n",
       "      <th>GWETPROF</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2M_MIN</th>\n",
       "      <th>ALLSKY_SFC_PAR_TOT</th>\n",
       "      <th>PRECTOTCORR</th>\n",
       "      <th>ALLSKY_SFC_SW_DNI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEH1_2022</td>\n",
       "      <td>20220101</td>\n",
       "      <td>9.70</td>\n",
       "      <td>100.62</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.89</td>\n",
       "      <td>97.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>13.69</td>\n",
       "      <td>17.08</td>\n",
       "      <td>13.51</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "      <td>13.89</td>\n",
       "      <td>10.15</td>\n",
       "      <td>10.18</td>\n",
       "      <td>4.59</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEH1_2022</td>\n",
       "      <td>20220102</td>\n",
       "      <td>9.70</td>\n",
       "      <td>100.44</td>\n",
       "      <td>3.66</td>\n",
       "      <td>2.45</td>\n",
       "      <td>93.00</td>\n",
       "      <td>0.66</td>\n",
       "      <td>13.93</td>\n",
       "      <td>16.78</td>\n",
       "      <td>13.37</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.63</td>\n",
       "      <td>14.49</td>\n",
       "      <td>8.79</td>\n",
       "      <td>13.28</td>\n",
       "      <td>14.32</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEH1_2022</td>\n",
       "      <td>20220103</td>\n",
       "      <td>3.30</td>\n",
       "      <td>101.48</td>\n",
       "      <td>6.94</td>\n",
       "      <td>2.30</td>\n",
       "      <td>84.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>7.39</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-5.74</td>\n",
       "      <td>12.02</td>\n",
       "      <td>25.85</td>\n",
       "      <td>2.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEH1_2022</td>\n",
       "      <td>20220104</td>\n",
       "      <td>2.14</td>\n",
       "      <td>102.81</td>\n",
       "      <td>1.91</td>\n",
       "      <td>10.73</td>\n",
       "      <td>85.75</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-6.02</td>\n",
       "      <td>1.01</td>\n",
       "      <td>-7.28</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-4.77</td>\n",
       "      <td>-9.42</td>\n",
       "      <td>51.66</td>\n",
       "      <td>0.01</td>\n",
       "      <td>25.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DEH1_2022</td>\n",
       "      <td>20220105</td>\n",
       "      <td>4.76</td>\n",
       "      <td>101.36</td>\n",
       "      <td>2.99</td>\n",
       "      <td>5.67</td>\n",
       "      <td>98.44</td>\n",
       "      <td>0.70</td>\n",
       "      <td>3.15</td>\n",
       "      <td>5.08</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>3.21</td>\n",
       "      <td>-4.70</td>\n",
       "      <td>28.96</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>WIH3_2022</td>\n",
       "      <td>20221106</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160</th>\n",
       "      <td>WIH3_2022</td>\n",
       "      <td>20221107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>WIH3_2022</td>\n",
       "      <td>20221108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>WIH3_2022</td>\n",
       "      <td>20221109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>WIH3_2022</td>\n",
       "      <td>20221110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8164 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Env      Date  QV2M      PS  WS2M  ALLSKY_SFC_SW_DWN   RH2M  \\\n",
       "0     DEH1_2022  20220101  9.70  100.62  3.25               1.89  97.00   \n",
       "1     DEH1_2022  20220102  9.70  100.44  3.66               2.45  93.00   \n",
       "2     DEH1_2022  20220103  3.30  101.48  6.94               2.30  84.56   \n",
       "3     DEH1_2022  20220104  2.14  102.81  1.91              10.73  85.75   \n",
       "4     DEH1_2022  20220105  4.76  101.36  2.99               5.67  98.44   \n",
       "...         ...       ...   ...     ...   ...                ...    ...   \n",
       "8159  WIH3_2022  20221106   NaN     NaN   NaN                NaN    NaN   \n",
       "8160  WIH3_2022  20221107   NaN     NaN   NaN                NaN    NaN   \n",
       "8161  WIH3_2022  20221108   NaN     NaN   NaN                NaN    NaN   \n",
       "8162  WIH3_2022  20221109   NaN     NaN   NaN                NaN    NaN   \n",
       "8163  WIH3_2022  20221110   NaN     NaN   NaN                NaN    NaN   \n",
       "\n",
       "      GWETTOP  T2MWET  T2M_MAX  T2MDEW  GWETROOT  GWETPROF    T2M  T2M_MIN  \\\n",
       "0        0.61   13.69    17.08   13.51      0.62      0.61  13.89    10.15   \n",
       "1        0.66   13.93    16.78   13.37      0.65      0.63  14.49     8.79   \n",
       "2        0.67   -0.85     7.39   -2.04      0.66      0.65   0.33    -5.74   \n",
       "3        0.66   -6.02     1.01   -7.28      0.67      0.65  -4.77    -9.42   \n",
       "4        0.70    3.15     5.08    3.10      0.68      0.66   3.21    -4.70   \n",
       "...       ...     ...      ...     ...       ...       ...    ...      ...   \n",
       "8159      NaN     NaN      NaN     NaN       NaN       NaN    NaN      NaN   \n",
       "8160      NaN     NaN      NaN     NaN       NaN       NaN    NaN      NaN   \n",
       "8161      NaN     NaN      NaN     NaN       NaN       NaN    NaN      NaN   \n",
       "8162      NaN     NaN      NaN     NaN       NaN       NaN    NaN      NaN   \n",
       "8163      NaN     NaN      NaN     NaN       NaN       NaN    NaN      NaN   \n",
       "\n",
       "      ALLSKY_SFC_PAR_TOT  PRECTOTCORR  ALLSKY_SFC_SW_DNI  \n",
       "0                  10.18         4.59               1.30  \n",
       "1                  13.28        14.32               1.43  \n",
       "2                  12.02        25.85               2.05  \n",
       "3                  51.66         0.01              25.53  \n",
       "4                  28.96         2.93               2.68  \n",
       "...                  ...          ...                ...  \n",
       "8159                 NaN          NaN                NaN  \n",
       "8160                 NaN          NaN                NaN  \n",
       "8161                 NaN          NaN                NaN  \n",
       "8162                 NaN          NaN                NaN  \n",
       "8163                 NaN          NaN                NaN  \n",
       "\n",
       "[8164 rows x 18 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_weather_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dfffd-fb98-419f-97a8-9dcb3c390b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('./data/Testing_Data/1_Submission_Template_2022.csv')\n",
    "#impute missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "col_order = weather_data_clean.columns\n",
    "test_weather_data_raw = pd.read_csv('./data/Testing_Data/4_Testing_Weather_Data_2022.csv')\n",
    "test_weather_data_raw = test_weather_data_raw.reindex(columns=col_order)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(test_weather_data_raw.select_dtypes('float'))\n",
    "test_weather_data = imputer.transform(test_weather_data_raw.select_dtypes('float'))\n",
    "test_weather_array = W.scaler.transform(test_weather_data)\n",
    "\n",
    "wet_dict = {}\n",
    "for i in set(submission['Env']):\n",
    "  env_index = np.array(list(test_weather_data_raw.loc[test_weather_data_raw['Env'] == i].index[:300]))\n",
    "  wet_dict[i] = test_weather_array[env_index,:]\n",
    "    \n",
    "#for i in range(submission.shape[0]):\n",
    "preds = []\n",
    "for i in range(submission.shape[0]):\n",
    "  ENV, HYBRID = submission.iloc[i]['Env'], submission.iloc[i]['Hybrid']\n",
    "  genotype_index = np.where(snp_data[0]==\"BGEM-0124-N/LH244\")[0][0]\n",
    "  g = snp_data[1][:,167]\n",
    "  w = wet_dict[ENV]\n",
    "\n",
    "  g = torch.tensor(g)\n",
    "  w = torch.tensor(w)\n",
    "\n",
    "  gin = g.view(1,1,g.shape[0]).type(torch.float32)\n",
    "  win = w.view(1,w.shape[0], w.shape[1]).type(torch.float32)\n",
    "\n",
    "  preds.append(model((gin,win)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc8fbe-f91e-426d-917f-161610d9a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08deea-2a6e-4970-991f-8443520dee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_template = './data/Testing_Data/1_Submission_Template_2022.csv'\n",
    "test_weather = './data/Testing_Data/4_Testing_Weather_Data_2022.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11645a-6c79-4375-a3b8-b9c29db79671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "test_weather_data_raw = pd.read_csv(test_weather)\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(test_weather_data_raw.select_dtypes('float'))\n",
    "test_weather_data = imputer.transform(test_weather_data)\n",
    "\n",
    "test_weather_array = W.scaler.transform(test_weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a6076-e991-47ad-a0d0-9f0a76592180",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8711a-0863-4b2d-82fe-6760ea1b00ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = pd.read_csv(sub_template).iloc[3000,:][\"Env\"]\n",
    "test_weather_array[test_weather_data_raw.loc[test_weather_data_raw['Env'] == test_env][:300].index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f474a6-476d-444e-9d23-6921230463e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pd.read_csv(sub_template)['Env']:\n",
    "    test_weather_array[test_weather_data_raw.loc[test_weather_data_raw['Env'] == i][:300].index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d21817-e635-4263-939d-25ba4c57b0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287df85-4d81-4d1d-8f3d-a18c8b757224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a155d-ea73-42f5-a5a3-33be00fe3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weather_data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe96ee-0c0a-4457-8b8f-b096fdb3824c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da805115-5e83-4c15-ae1d-016ade890fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(test_weather).head(50).select_dtypes('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15811ab4-1c72-4489-aacf-fb56129b1d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6cad2-babe-4a17-b4d5-bb2b991f3a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192be403-98ba-4443-a480-43eb0b9b819b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916ed2c-f9d4-4b04-976c-361bdc4aabc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7546f60-c73b-421b-af9d-0a1a6a284cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d4621-75bf-4363-b751-f48d1189d7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701747b-b82e-4257-933e-b029584f8c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc572e-4854-450f-a74e-01b3a4db8dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70397bdb-f7fd-4be1-8a45-eb1394913496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a6042-5ef9-4c34-8fdc-59f7cb892a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d685b3-ef0e-407d-a98e-5ac5cabe3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet2(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1, groups=128, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EfficientNet3(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet3, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1,  bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class NNEnsemble3(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list, alpha=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.lin1 = torch.nn.Linear(hidden_list[0], hidden_list[1])\n",
    "    self.lin2 = torch.nn.Linear(hidden_list[1], hidden_list[2])\n",
    "    self.out = torch.nn.Linear(hidden_list[2], 1)\n",
    "\n",
    "    \n",
    "    if alpha != None:\n",
    "        print('X')\n",
    "        for c,l in enumerate(self.layers):\n",
    "            print(l)\n",
    "            torch.nn.init.xavier_normal_(l.weight,gain=alpha)\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    " #   print(g.shape, w.shape)\n",
    "\n",
    "    if w.dim() == 3:\n",
    "      w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    if g.dim() == 3:\n",
    "      g = g.view(g.shape[0], g.shape[1] * g.shape[2])\n",
    "\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = torch.nn.functional.relu(x) \n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1237715-53ff-4101-93e3-de07ebbcf5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_list, dummy, alpha=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create a list of linear layers, with the correct input and output dimensions\n",
    "        self.layers = torch.nn.ModuleList([nn.LazyLinear(x) for x in hidden_list])\n",
    "        \n",
    "        \n",
    "        if alpha != None:\n",
    "            x = dummy\n",
    "            for l in self.layers:\n",
    "                if x.dim() == 3:\n",
    "                    x = x.view(x.shape[0], x.shape[1]*x.shape[2])\n",
    "\n",
    "                x = l(x)\n",
    "                torch.nn.init.xavier_normal_(l.weight,gain=hidden_list[0] / 100)\n",
    "\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the dropout layer to the input\n",
    "        #x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "        if x.dim() == 3:\n",
    "            x = x.view(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        # Iterate through the linear layers, applying each one to the input\n",
    "        for c, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "            if c < len(self.layers)-1:\n",
    "              x = torch.nn.functional.relu(x)  \n",
    "            \n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "class NNEnsemble(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list, alpha=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.lin1 = torch.nn.Linear(hidden_list[0], hidden_list[1])\n",
    "    self.lin2 = torch.nn.Linear(hidden_list[1], hidden_list[2])\n",
    "    self.out = torch.nn.Linear(hidden_list[2], 1)\n",
    "\n",
    "    \n",
    "    if alpha != None:\n",
    "        print('X')\n",
    "        for c,l in enumerate(self.layers):\n",
    "            print(l)\n",
    "            torch.nn.init.xavier_normal_(l.weight,gain=alpha)\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    "\n",
    "    if w.dim() == 3:\n",
    "      w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = torch.nn.functional.relu(x) \n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feae10c-9406-421f-a9c6-439eab9f62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "gmodel = LazyMLP([500,50,25],2)\n",
    "wmodel = LazyMLP([5840,50,15],1)\n",
    "model = NNEnsemble([200,50,25], [gmodel,wmodel])\n",
    "#plt.hist(detach_list(model((g,w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba800b03-0e39-439a-889a-34ef49e8f5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmodel(w.type(torch.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680ea0e-8a79-4815-947c-ae9997e56cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249, 25])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel(g.type(torch.float32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d6312-0492-438a-961b-222e151c1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model((g.type(torch.float32), w.type(torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc03de-ea46-4f71-a484-2b07eb858f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "  0%|                                                                               | 1/5000 [00:01<2:06:25,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1828, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                           | 101/5000 [01:46<1:22:35,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(0.1603, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███                                                                          | 201/5000 [03:30<1:23:31,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 tensor(0.1387, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▋                                                                        | 301/5000 [05:17<1:16:57,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 tensor(0.1180, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▏                                                                      | 401/5000 [07:07<1:29:39,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 tensor(0.0996, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 501/5000 [08:55<1:26:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 tensor(0.0834, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▎                                                                   | 601/5000 [10:47<1:16:11,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 tensor(0.0693, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▊                                                                  | 701/5000 [12:35<1:18:33,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 tensor(0.0571, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▎                                                                | 801/5000 [14:22<1:15:11,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 tensor(0.0464, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▉                                                               | 901/5000 [16:09<1:11:06,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 tensor(0.0379, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▏                                                            | 1001/5000 [17:56<1:11:37,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 tensor(0.0316, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▋                                                           | 1101/5000 [19:48<1:13:47,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100 tensor(0.0256, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████▎                                                         | 1201/5000 [21:39<1:06:02,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 tensor(0.0223, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████▊                                                        | 1301/5000 [23:40<1:10:22,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 tensor(0.0198, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████▎                                                      | 1401/5000 [25:30<1:09:01,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 tensor(0.0176, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██████████████████████▊                                                     | 1501/5000 [27:18<1:01:43,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 tensor(0.0160, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▎                                                   | 1601/5000 [29:14<1:00:27,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 tensor(0.0152, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████▊                                                  | 1701/5000 [31:05<1:08:13,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 tensor(0.0138, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|████████████████████████████                                                  | 1801/5000 [33:02<59:58,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 tensor(0.0132, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▋                                                | 1900/5000 [34:51<56:53,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1900 tensor(0.0133, grad_fn=<HuberLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# gm = EfficientNet3(num_classes=1500)\n",
    "# wm = EfficientNet3(300, num_classes=1500)\n",
    "                    \n",
    "gmodel = LazyMLP([500,50,25],2)\n",
    "wmodel = LazyMLP([5840,50,15],1)\n",
    "model = NNEnsemble([200,100,50], [gmodel,wmodel])\n",
    "#plt.hist(detach_list(model((g,w))))\n",
    "\n",
    "# model = NNEnsemble3([2000,1000,1000], [gm,wm],alpha=2)\n",
    "# # plt.hist(detach_list(model((g,w))))\n",
    "\n",
    "opt = optim.Adamax(model.parameters(), lr=.0001, weight_decay=.0001)\n",
    "#scheduler =  optim.lr_scheduler.ReduceLROnPlateau(opt)\n",
    "\n",
    "loss_func = torch.nn.functional.huber_loss\n",
    "\n",
    "tr_loss = [] \n",
    "te_loss = []\n",
    "te_MA = []\n",
    "predicts = []\n",
    "targets = []\n",
    "\n",
    "#for i in tqdm(range(len(tr_dataloader))):\n",
    "for i in tqdm(range(5000)):\n",
    "  model.train()\n",
    "  #train loop\n",
    "  y,g,w = next(iter(tr_dataloader))\n",
    "  y =  y[:,-1]\n",
    "  y = y.type(torch.float32)\n",
    "  g = g.type(torch.float32)\n",
    "  w = w.type(torch.float32)\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y, preds)\n",
    "\n",
    "  loss.backward()\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "  #scheduler.step(loss)\n",
    "  tr_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "  #test loop\n",
    "  model.eval()\n",
    "  y,g,w = next(iter(te_dataloader))\n",
    "  y =  y[:,-1]\n",
    "  y = y.to(torch.float32)\n",
    "  g = g.to(torch.float32)\n",
    "  w = w.to(torch.float32)\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y,preds)\n",
    "\n",
    "  te_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "  if i % 100 == 0:\n",
    "    print(i, loss)\n",
    "    te_MA.append(loss)\n",
    "    if (len(te_MA) > 1) and ((te_MA[len(te_MA)-1] > te_MA[len(te_MA)-2])):\n",
    "      break\n",
    "\n",
    "  \n",
    "  \n",
    "# plt.plot(tr_loss)\n",
    "# plt.plot(te_loss)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bda822-acd7-46a9-ac8f-f66c395165a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8937774\n"
     ]
    }
   ],
   "source": [
    "ys = []\n",
    "predicted = []\n",
    "for y,g,w in te_dataloader:\n",
    "    model.eval()\n",
    "    preds = model((g.type(torch.float32),w.type(torch.float32)))\n",
    "    \n",
    "    for i in preds:\n",
    "        predicted.append(i.detach().numpy())\n",
    "    for x in y:\n",
    "        ys.append(x.detach().numpy())\n",
    "        \n",
    "mypred = np.array(predicted).ravel()\n",
    "mytarget = np.array(ys)\n",
    "\n",
    "\n",
    "rmse = []\n",
    "for p,t in zip(mypred,mytarget):\n",
    "    if not np.isnan(t[-1]) or np.isnan(p):\n",
    "        target_scaled = (Y.scaler.inverse_transform(np.array(t).reshape(1,-1))[0][-1])\n",
    "        pred_scaled = (Y.scaler.inverse_transform(np.array(p).reshape(-1,1)))\n",
    "        rmse.append(np.sqrt((target_scaled-pred_scaled)**2))\n",
    "    else:\n",
    "        pass\n",
    "print(np.mean(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412c4bb-d1e7-4f93-96b3-024edff615fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f6ef1-2832-4756-a87a-891384c6d9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c8d7b-7bb2-4de5-a908-ced094d0bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNforA(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1, groups=128, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fd537-a8bf-4f6e-a51c-ffa4ffaa172a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b867e9-e172-4d29-9b91-e5afb7e45bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "#| export\n",
    "class newGemDataset():\n",
    "    \"\"\"\n",
    "    Pytorch Dataset which can be used with dataloaders for simple batching during training loops\n",
    "    \"\"\"\n",
    "    def __init__(self,W,Y,G, def_device='cpu'):\n",
    "        self.W = W\n",
    "        self.SNP = G\n",
    "        self.Y = Y\n",
    "        self.device = def_device\n",
    "        \n",
    "    def __len__(self): return self.Y[0].shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "      y = self.Y[0][idx]\n",
    "      e = self.Y[1][idx]\n",
    "      h = self.Y[2][idx]\n",
    "      d = self.Y[3][idx]\n",
    "\n",
    "      #weather\n",
    "      w = self.W[1][np.where(self.W[0] == e)[0][0]]\n",
    "\n",
    "      #snp\n",
    "      g = snp_data[1][:,np.where(snp_data[0] == h)[0][0]]\n",
    "      return y,g,w\n",
    "\n",
    "\n",
    "#| export\n",
    "class ST():\n",
    "    \"\"\"\n",
    "    A class which will hold the secondary trait data for the entire dataset for pre-training purposes\n",
    "    \n",
    "    init\n",
    "        yield_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, yield_data, testYear):\n",
    "\n",
    "        self.Te = yield_data.iloc[([str(testYear) in x for x in yield_data['Env']]),:].reset_index()\n",
    "        self.Tr = yield_data.iloc[([str(testYear) not in x for x in yield_data['Env']]),:].reset_index()\n",
    "\n",
    "        self.secondary_traits = [\n",
    "                'Stand_Count_plants',\n",
    "                'Pollen_DAP_days',\n",
    "                'Silk_DAP_days',\n",
    "                'Plant_Height_cm',\n",
    "                'Ear_Height_cm',\n",
    "                #'Root_Lodging_plants',\n",
    "                #'Stalk_Lodging_plants',\n",
    "                'Twt_kg_m3',\n",
    "                'Yield_Mg_ha',\n",
    "                #'Date_Harvested'\n",
    "                ]\n",
    "        \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_arrays(self.Tr)\n",
    "        self.make_arrays(self.Te, False)\n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(np.array(self.Tr[self.secondary_traits]))\n",
    "        self.scaler = ss\n",
    "\n",
    "    def scale_data(self,df):\n",
    "        scaled_secondary = self.scaler.transform(np.array(df[self.secondary_traits]))\n",
    "        for c,i in enumerate(self.secondary_traits):\n",
    "            #print(i)\n",
    "            df[i] = scaled_secondary[:,c]\n",
    "    \n",
    "    def plot_yields(self):\n",
    "        for i in self.secondary_traits:\n",
    "            plt.hist(self.Tr[i],density=True, label='Train',alpha=.5,bins=50)\n",
    "            plt.hist(self.Te[i],density=True, label='Test',alpha=.5,bins=50)\n",
    "            plt.legend()\n",
    "            plt.title(i)\n",
    "            plt.show()\n",
    "\n",
    "    def make_arrays(self,df,train=True):\n",
    "      df = np.array(df[self.secondary_traits]), np.array(df['Env']) , np.array(df['Hybrid']), np.array(df['Date_Planted'])\n",
    "      if train:\n",
    "        self.Tr = df\n",
    "      else:\n",
    "        self.Te= df\n",
    "\n",
    "#| export\n",
    "class newWT():\n",
    "    \"\"\"\n",
    "    A class which will hold the weather data for the entire dataset for training purposes\n",
    "    \n",
    "    init\n",
    "        weather_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, weather_data, testYear):\n",
    "        \n",
    "        self.Te = weather_data.iloc[([str(testYear) in x for x in weather_data['Year']]),:].reset_index()\n",
    "        self.Tr = weather_data.iloc[([str(testYear) not in x for x in weather_data['Year']]),:].reset_index()\n",
    "            \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_array(self.Tr)\n",
    "        self.make_array(self.Te,False)\n",
    "            \n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(self.Tr.select_dtypes('float'))\n",
    "        self.scaler = ss\n",
    "            \n",
    "    def scale_data(self, df):\n",
    "        fd = df.select_dtypes('float')\n",
    "        fs = self.scaler.transform(fd)\n",
    "        df[fd.columns] = fs\n",
    "\n",
    "    def make_array(self, df,train = True):\n",
    "      for c,i in enumerate(set(df['Env'])):\n",
    "        env_weather = np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "        #print(env_weather.shape)\n",
    "        if c == 0:\n",
    "          env_order = list([i])\n",
    "          weather_array =   np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "          weather_array = np.expand_dims(weather_array,axis=0)\n",
    "        else:\n",
    "          weather_array = np.vstack((weather_array, env_weather[None,:,:]))\n",
    "          env_order.append(i)\n",
    "\n",
    "        if train:\n",
    "          self.Tr = (np.array(env_order), np.array(weather_array))\n",
    "        else:\n",
    "          self.Te = (np.array(env_order), np.array(weather_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

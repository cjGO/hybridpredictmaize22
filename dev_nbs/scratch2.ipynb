{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d3c97-251c-4c48-a897-72ab492c428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2901e4-da46-4587-8c42-e4b3d4272c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def collect_snps(method):\n",
    "    \"\"\"\n",
    "    Input\n",
    "        method: a Path(PosixPath) to directory containing npy arrays for each chr\n",
    "    Output\n",
    "        tuple(hybrid ids, snp matrix)\n",
    "    \"\"\"\n",
    "    for c,chr in enumerate(method.iterdir()):\n",
    "        if c == 0:\n",
    "            strains,snp_data = np.load(chr,allow_pickle=True)\n",
    "        else:\n",
    "            strains, snps = np.load(chr, allow_pickle=True)\n",
    "            snp_data = np.vstack((snp_data,snps))\n",
    "\n",
    "            return strains,snp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf6b080-82be-4555-bb73-5f19f0e83ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WT():\n",
    "    \"\"\"\n",
    "    A class which will hold the weather data for the entire dataset for training purposes\n",
    "    \n",
    "    init\n",
    "        weather_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, weather_data, testYear):\n",
    "        \n",
    "        self.Te = weather_data.iloc[([str(testYear) in x for x in weather_data['Year']]),:].reset_index()\n",
    "        self.Tr = weather_data.iloc[([str(testYear) not in x for x in weather_data['Year']]),:].reset_index()\n",
    "            \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "            \n",
    "    def setup_scaler(self):\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(self.Tr.select_dtypes('float'))\n",
    "        self.scaler = ss\n",
    "            \n",
    "    def scale_data(self, df):\n",
    "        fd = df.select_dtypes('float')\n",
    "        fs = self.scaler.transform(fd)\n",
    "        df[fd.columns] = fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59db58-1330-4d5a-9c8f-1d67127f696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class YT():\n",
    "    \"\"\"\n",
    "    A class which will hold the yield data for the entire dataset for training purposes\n",
    "    \n",
    "    init\n",
    "        yield_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, yield_data, testYear):\n",
    "\n",
    "        self.Te = yield_data.iloc[([str(testYear) in x for x in yield_data['Env']]),:].reset_index()\n",
    "        self.Tr = yield_data.iloc[([str(testYear) not in x for x in yield_data['Env']]),:].reset_index()\n",
    "\n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "    def setup_scaler(self):\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(np.array(self.Tr['Twt_kg_m3']).reshape(-1,1))\n",
    "        self.scaler = ss\n",
    "\n",
    "    def scale_data(self,df):\n",
    "        ya = np.array(df['Twt_kg_m3']).reshape(-1,1)\n",
    "        ys = self.scaler.transform(ya)\n",
    "        df['scaled_yield'] = ys\n",
    "\n",
    "    def plot_yields(self):\n",
    "\n",
    "        plt.hist(self.Tr['scaled_yield'],density=True, label='Train',alpha=.5,bins=50)\n",
    "        plt.hist(self.Te['scaled_yield'],density=True, label='Test',alpha=.5,bins=50)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e364cd-a1bf-47d5-8464-729b8e31b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GEM():\n",
    "    \"\"\"\n",
    "    init\n",
    "        split -> the year that will be designated as the test split\n",
    "    \"\"\"\n",
    "    def __init__(self, split):\n",
    "        self.split = str(split)\n",
    "        self.Y = None\n",
    "        self.W = None\n",
    "        self.SNP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171a4cd9-3f3c-4dc6-af18-0e0acf86787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "class GemDataset():\n",
    "    \"\"\"\n",
    "    Pytorch Dataset which can be used with dataloaders for simple batching during training loops\n",
    "    \"\"\"\n",
    "    def __init__(self,W,Y,G):\n",
    "        self.W = W\n",
    "        self.SNP = G\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self): return self.Y.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        #keys to access SNPs and Weather\n",
    "        hybrid = self.Y.iloc[idx,:]['Hybrid']\n",
    "        env = self.Y.iloc[idx,:]['Env']\n",
    "\n",
    "        #values for the model training\n",
    "        target = self.Y.iloc[idx,:]['scaled_yield']\n",
    "        genotype = self.SNP[1][:, np.where(self.SNP[0]==hybrid)[0][0]]\n",
    "        weather = np.array(self.W.loc[self.W['Env'] == env].select_dtypes('float'))\n",
    "\n",
    "        return target, genotype, weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15730f-8e62-4e4d-b7be-bd48405f7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def remove_leapdays(weather):\n",
    "    \"\"\" just a hotfix \"\"\"\n",
    "    to_remove = []\n",
    "    for i in list(set(weather_data['Env'])):\n",
    "        if (sum(weather_data['Env'] == i)) == 366:\n",
    "            #get indexes\n",
    "            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))\n",
    "    return weather_data.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00eff1-f1b9-4afa-b484-95fc84334bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('data/snpCompress/')\n",
    "data_path = Path('data/')\n",
    "path_train_weatherTable =data_path/'Training_Data/4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'Training_Data/1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_50'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b144e-c39b-4eac-8e9f-b94a0f9a0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e888d7e-de58-4ba6-b69c-b52ae9b9e316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP(input_size=100, hidden_size=64, output_size=1)\n",
    "\n",
    "# Generate some random input data\n",
    "input_data = torch.randn(32, 100)\n",
    "\n",
    "# Make a prediction\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e193a88-a1f3-47e6-af8f-3dd61ed36b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import tensor,nn,optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b31919-72fb-489a-a975-36c07405d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a GEM dataset\n",
    "gem = GEM(test_split)\n",
    "\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dls = (tr_dataloader, te_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342497a3-8578-402f-809d-371952d628b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e752c1-7a20-43fd-b35f-9cff6d2b4374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "501\n"
     ]
    }
   ],
   "source": [
    "\n",
    "losses = []\n",
    "model = MLP(100,100,1)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=.05)\n",
    "loss_func = F.mse_loss\n",
    "for c, (y,g,w) in enumerate(tr_dataloader):\n",
    "    g = g.type(torch.float32)\n",
    "    #print(g.shape)\n",
    "    preds = model(g)\n",
    "    y = y.type(torch.float32)\n",
    "    loss = loss_func(preds.squeeze(),y)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    #print(preds[0])\n",
    "    losses.append(loss.detach().numpy())\n",
    "    \n",
    "    if preds[0].isnan()[0] == True:\n",
    "        print('nan')\n",
    "    \n",
    "    if c > 500:\n",
    "        print(c)\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fe8f7-41af-442e-88a7-5ea5ff0bee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe829-6798-49c2-8db8-c79942904474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc119d-ce00-4181-bfbf-793096d6b177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

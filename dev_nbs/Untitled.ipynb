{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206da45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hybridpredictmaize22.hybridpredictmaize22.snpCompression import *\n",
    "from hybridpredictmaize22.GEMlearn import *\n",
    "from hybridpredictmaize22.GEMdataset import *\n",
    "from hybridpredictmaize22.snpCompression import *\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from warnings import warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb35f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('./data/snpCompress/')\n",
    "data_path = Path('./data/Training_Data/')\n",
    "path_train_weatherTable =data_path/'4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_100'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4c5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('data/Training_Data/1_Training_Trait_Data_2014_2021.csv')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_train_yieldTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9afe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/cltng/OneDrive/Documents/Projects/hybridpredictmaize22/dev_nbs\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ea9dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(Path('./data/snpCompress/PCS_50')) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bb8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "gem = GEM(test_split)\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466714e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_list, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create a list of linear layers, with the correct input and output dimensions\n",
    "        self.layers = torch.nn.ModuleList([nn.LazyLinear(x) for x in hidden_list])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the dropout layer to the input\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Iterate through the linear layers, applying each one to the input\n",
    "        for c, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "            if c < len(self.layers)-1:\n",
    "              x = torch.nn.functional.relu(x)  \n",
    "            \n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class NNEnsemble(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "\n",
    "    for x in hidden_list:\n",
    "      self.layers.append(nn.LazyLinear(x))\n",
    "\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    "\n",
    "    w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      if c < len(self.layers)-1:\n",
    "        x = torch.nn.functional.relu(x) \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8ee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:40<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "gmodel = LazyMLP([500,100,10])\n",
    "wmodel = LazyMLP([5840,100,50])\n",
    "\n",
    "model = NNEnsemble([100,50,1], [gmodel,wmodel])\n",
    "model = model\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=.05)\n",
    "loss_func = F.mse_loss\n",
    "\n",
    "tr_loss = [] \n",
    "\n",
    "te_loss = []\n",
    "\n",
    "#for i in tqdm(range(len(tr_dataloader))):\n",
    "for i in tqdm(range(500)):\n",
    "  \n",
    "  #train loop\n",
    "  y,g,w = next(iter(tr_dataloader))\n",
    "\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y,preds)\n",
    "\n",
    "  loss.backward()\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "\n",
    "  tr_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "  #test loop  \n",
    "  y,g,w = next(iter(te_dataloader))\n",
    "\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y,preds)\n",
    "\n",
    "  te_loss.append(loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda883da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f3c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808e025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8eaf0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d023d4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e0480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD):\n",
    "        cbs = fc.L(cbs)\n",
    "        self.cb_ctx = partial(callback_ctx, self.callback)\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.cb_ctx('epoch'):\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                with self.cb_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()\n",
    "    \n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        # `add_cb` and `rm_cb` were added in lesson 18\n",
    "        for cb in cbs: self.cbs.append(cb)\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            self.opt = self.opt_func(self.model.parameters(), self.lr if lr is None else lr)\n",
    "            with self.cb_ctx('fit'):\n",
    "                for self.epoch in self.epochs:\n",
    "                    if train: self.one_epoch(True)\n",
    "                    if valid: torch.no_grad()(self.one_epoch)(False)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_list, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Create a list of linear layers, with the correct input and output dimensions\n",
    "        self.layers = torch.nn.ModuleList([nn.LazyLinear(x) for x in hidden_list])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the dropout layer to the input\n",
    "        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Iterate through the linear layers, applying each one to the input\n",
    "        for c, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            \n",
    "            if c < len(self.layers)-1:\n",
    "              x = torch.nn.functional.relu(x)  \n",
    "            \n",
    "            x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e945bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1bdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd77fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6744493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = LazyMLP([500,10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f2418",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCB(Callback):\n",
    "    def predict(self, learn):\n",
    "        learn.preds = learn.model(learn.batch[1])\n",
    "    def get_loss(self, learn):\n",
    "        learn.loss = learn.loss_func(learn.preds.squeeze(), learn.batch[0])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5381789",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,g,w = next(iter(tr_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, dls, cbs = [TrainCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937912fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4786],\n",
       "        [-0.7224],\n",
       "        [-0.5547],\n",
       "        [-0.2708],\n",
       "        [-0.3547],\n",
       "        [-0.6647],\n",
       "        [-0.3344],\n",
       "        [-0.4365],\n",
       "        [-0.4018],\n",
       "        [-0.5777],\n",
       "        [-0.6922],\n",
       "        [-0.3879],\n",
       "        [-0.3761],\n",
       "        [-0.3826],\n",
       "        [-0.3891],\n",
       "        [-0.5835],\n",
       "        [-0.1854],\n",
       "        [-0.7585],\n",
       "        [-0.5328],\n",
       "        [-0.3824],\n",
       "        [-0.4994],\n",
       "        [-0.4323],\n",
       "        [-0.5865],\n",
       "        [-0.2736],\n",
       "        [-0.3865],\n",
       "        [-0.2693],\n",
       "        [-0.7051],\n",
       "        [-0.4338],\n",
       "        [-0.5399],\n",
       "        [-0.6746],\n",
       "        [-0.6584],\n",
       "        [-0.5087]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebfc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

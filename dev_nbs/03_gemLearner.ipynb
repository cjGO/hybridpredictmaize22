{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMdataset\n",
    "\n",
    "> This package will hold the GxExM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "#| hide\n",
    "from hybridpredictmaize22.GEMdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def remove_leapdays(weather):\n",
    "    \"\"\" just a hotfix \"\"\"\n",
    "    to_remove = []\n",
    "    for i in list(set(weather_data['Env'])):\n",
    "        if (sum(weather_data['Env'] == i)) == 366:\n",
    "            #get indexes\n",
    "            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))\n",
    "    return weather_data.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = {\n",
    "'test_split':2019,\n",
    "'path_snps':Path('data/snpCompress/'),\n",
    "'data_path':Path('data/'),\n",
    "'path_train_weatherTable':Path('data/Training_Data/4_Training_Weather_Data_2014_2021.csv'),\n",
    "'path_train_yieldTable':Path('data/Training_Data/1_Training_Trait_Data_2014_2021.csv'),\n",
    "'snp_compression':'PCS_50',\n",
    "'batch_size':64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('data/snpCompress/')\n",
    "data_path = Path('data/')\n",
    "path_train_weatherTable =data_path/'Training_Data/4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'Training_Data/1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_50'\n",
    "batch_size = 64\n",
    "\n",
    "def setup_data():\n",
    "    \n",
    "    snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "    yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "    weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "    \n",
    "    yield_data = yield_data[yield_data['Twt_kg_m3'].notnull()] #Remove plots w/ missing yields\n",
    "    #yield_data = yield_data.reset_index()\n",
    "    weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "    #removes yield data where no weather data\n",
    "    setYield = set(yield_data['Env'])\n",
    "    setWeather = set(weather_data['Env'])\n",
    "    only_yield = setYield - setWeather\n",
    "    only_weather = setWeather - setYield\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "    #removes yield data where no genotype data\n",
    "    setSNP = set(snp_data[0])\n",
    "    setYield = set(yield_data['Hybrid'])\n",
    "    only_yield = setYield - setSNP\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "    weather_data = remove_leapdays(weather_data)\n",
    "    #weather_data = weather_data.reset_index()\n",
    "    #yield_data=yield_data.sample(frac=1)\n",
    "    yield_data = yield_data.reset_index()\n",
    "    \n",
    "    #Create a GEM dataset\n",
    "    gem = GEM(test_split)\n",
    "    gem.Y = YT(yield_data, test_split)\n",
    "    gem.W = WT(weather_data, test_split)\n",
    "    gem.SNP = snp_data\n",
    "    \n",
    "    tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "    te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "    \n",
    "    tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return tr_dataloader, te_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Twt_kg_m3'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "#yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()\n",
    "\n",
    "#Create a GEM dataset\n",
    "gem = GEM(test_split)\n",
    "\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_oz(layer):\n",
    "    inSize = layer.in_features\n",
    "    outSize = layer.out_features\n",
    "    layer.weight = torch.nn.Parameter(torch.tensor(np.array(np.random.choice([-1, 0, 1], size=(outSize,inSize)))).type(torch.float32))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gModel(nn.Module):\n",
    "    def __init__(self, inSide,hiddenSize):\n",
    "        super().__init__()\n",
    "        #self.in = nn.Linear()\n",
    "        self.inputLayer = init_oz(nn.Linear(hiddenSize,inSide))\n",
    "        self.hiddenLayer = init_oz(nn.Linear(hiddenSize,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.inputLayer(x)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = init_oz(nn.Linear(input_size, hidden_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = init_oz(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create an instance of the MLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unhook(t):\n",
    "    return t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaders:\n",
    "    def __init__(self, dls):\n",
    "        #print(dls)\n",
    "        self.train,self.valid = dls[:2]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        #method_nm ({before,after} {batch,epoch,fit})\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=torch.optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):        \n",
    "        #my edit\n",
    "        self.yb, self.gb, self.wb = [to_device(x).type(torch.float32) for x in self.batch]\n",
    "        \n",
    "        self.preds = self.model(self.gb).squeeze()\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        try:\n",
    "            self.callback('before_epoch')\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                try:\n",
    "                    self.callback('before_batch')\n",
    "                    self.one_batch()\n",
    "                    self.callback('after_batch')\n",
    "                except CancelBatchException: pass\n",
    "            self.callback('after_epoch')\n",
    "        except CancelEpochException: pass\n",
    "        \n",
    "    def fit(self, n_epochs):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.epochs = range(n_epochs)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        try:\n",
    "            self.callback('before_fit')\n",
    "            for self.epoch in self.epochs:\n",
    "                self.one_epoch(True)\n",
    "                self.one_epoch(False)\n",
    "            self.callback('after_fit')\n",
    "        except CancelFitException: pass\n",
    "        \n",
    "        #note the self here refers to the Learner object of the class\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "        \n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(x): return x.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopBatch(Callback):\n",
    "    def before_fit(self,learn): self.count=0\n",
    "    def after_batch(self,learn):\n",
    "        self.count+=1\n",
    "        if self.count > 10:\n",
    "            raise CancelEpochException()\n",
    "            \n",
    "            \n",
    "    def after_epoch(self,learn): pass\n",
    "    def after_fit(self, learn): print('toast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastcore.all as fc\n",
    "from operator import attrgetter,itemgetter\n",
    "\n",
    "model = MLP(input_size=100, hidden_size=2, output_size=1)#opt = Optimizer(gw.parameters(),lr=0.001)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_func = nn.MSELoss()\n",
    "dls = setup_data()\n",
    "lr=.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = Learner(model, DataLoaders(dls), loss_func, lr,  cbs=[CompletionCB(), StopBatch()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 12 batches\n",
      "toast\n"
     ]
    }
   ],
   "source": [
    "def_device = 'cpu'\n",
    "x.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self): self.reset()\n",
    "    def reset(self): self.vals,self.ns = [],[]\n",
    "    def add(self, inp, targ=None, n=1):\n",
    "        self.last = self.calc(inp, targ)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(n)\n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = tensor(self.ns)\n",
    "        return (tensor(self.vals)*ns).sum()/ns.sum()\n",
    "    def calc(self, inps, targs): return inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torcheval.metrics import MeanSquaredError, Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = MeanSquaredError()\n",
    "metric.update(torch.tensor([0, 2, 1, 3]), torch.tensor([0, 1, 2, 3]))\n",
    "metric.compute().sqrt()\n",
    "metric.reset()\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from copy import copy\n",
    "import torch.nn.functional as F\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class MetricsCB(Callback):\n",
    "    \n",
    "    def __init__(self, *ms, **metrics):\n",
    "            #this block just unpacks the init args\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        y,g,w = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricsCB(accuracy=MeanSquaredError())\n",
    "#|export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '207.336', 'loss': '-0.619', 'epoch': 0, 'train': 'train'}\n",
      "{'accuracy': '0.565', 'loss': '-25.647', 'epoch': 0, 'train': 'eval'}\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size=100, hidden_size=2, output_size=1)#opt = Optimizer(gw.parameters(),lr=0.001)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_func = nn.MSELoss()\n",
    "metrics = MetricsCB(accuracy=MeanSquaredError())\n",
    "learn = Learner(model, DataLoaders(dls), F.cross_entropy, lr=0.2, cbs=[metrics])\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMdataset\n",
    "\n",
    "> This package will hold the GxExM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "#| hide\n",
    "from hybridpredictmaize22.GEMdataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def remove_leapdays(weather):\n",
    "    \"\"\" just a hotfix \"\"\"\n",
    "    to_remove = []\n",
    "    for i in list(set(weather_data['Env'])):\n",
    "        if (sum(weather_data['Env'] == i)) == 366:\n",
    "            #get indexes\n",
    "            to_remove.append(max(list(weather_data.loc[weather_data['Env'] == i].index)))\n",
    "    return weather_data.drop(to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_split 2019\n",
      "path_snps data/snpCompress\n",
      "data_path data\n",
      "path_train_weatherTable data/Training_Data/4_Training_Weather_Data_2014_2021.csv\n",
      "path_train_yieldTable data/Training_Data/1_Training_Trait_Data_2014_2021.csv\n",
      "snp_compression PCS_50\n",
      "batch_size 64\n"
     ]
    }
   ],
   "source": [
    "ARGS = {\n",
    "'test_split':2019,\n",
    "'path_snps':Path('data/snpCompress/'),\n",
    "'data_path':Path('data/'),\n",
    "'path_train_weatherTable':Path('data/Training_Data/4_Training_Weather_Data_2014_2021.csv'),\n",
    "'path_train_yieldTable':Path('data/Training_Data/1_Training_Trait_Data_2014_2021.csv'),\n",
    "'snp_compression':'PCS_50',\n",
    "'batch_size':64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'test_split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mARGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_split\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'test_split'"
     ]
    }
   ],
   "source": [
    "ARGS.test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "path_snps = Path('data/snpCompress/')\n",
    "data_path = Path('data/')\n",
    "path_train_weatherTable =data_path/'Training_Data/4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'Training_Data/1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_50'\n",
    "batch_size = 64\n",
    "\n",
    "def setup_data():\n",
    "    \n",
    "    snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "    yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "    weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "    \n",
    "    yield_data = yield_data[yield_data['Twt_kg_m3'].notnull()] #Remove plots w/ missing yields\n",
    "    #yield_data = yield_data.reset_index()\n",
    "    weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "    #removes yield data where no weather data\n",
    "    setYield = set(yield_data['Env'])\n",
    "    setWeather = set(weather_data['Env'])\n",
    "    only_yield = setYield - setWeather\n",
    "    only_weather = setWeather - setYield\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "    #removes yield data where no genotype data\n",
    "    setSNP = set(snp_data[0])\n",
    "    setYield = set(yield_data['Hybrid'])\n",
    "    only_yield = setYield - setSNP\n",
    "    yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "    weather_data = remove_leapdays(weather_data)\n",
    "    #weather_data = weather_data.reset_index()\n",
    "    #yield_data=yield_data.sample(frac=1)\n",
    "    yield_data = yield_data.reset_index()\n",
    "    \n",
    "    #Create a GEM dataset\n",
    "    gem = GEM(test_split)\n",
    "    gem.Y = YT(yield_data, test_split)\n",
    "    gem.W = WT(weather_data, test_split)\n",
    "    gem.SNP = snp_data\n",
    "    \n",
    "    tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "    te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "    \n",
    "    tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "    te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return tr_dataloader, te_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = collect_snps(path_snps/snp_compression) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Twt_kg_m3'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n",
    "weather_data = remove_leapdays(weather_data)\n",
    "weather_data = weather_data.reset_index()\n",
    "#yield_data=yield_data.sample(frac=1)\n",
    "yield_data = yield_data.reset_index()\n",
    "\n",
    "#Create a GEM dataset\n",
    "gem = GEM(test_split)\n",
    "\n",
    "gem.Y = YT(yield_data, test_split)\n",
    "gem.W = WT(weather_data, test_split)\n",
    "gem.SNP = snp_data\n",
    "\n",
    "tr_ds = GemDataset(gem.W.Tr, gem.Y.Tr, gem.SNP)\n",
    "te_ds = GemDataset(gem.W.Te, gem.Y.Te, gem.SNP)\n",
    "\n",
    "tr_dataloader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True)\n",
    "te_dataloader = DataLoader(te_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_oz(layer):\n",
    "    inSize = layer.in_features\n",
    "    outSize = layer.out_features\n",
    "    layer.weight = torch.nn.Parameter(torch.tensor(np.array(np.random.choice([-1, 0, 1], size=(outSize,inSize)))).type(torch.float32))\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gModel(nn.Module):\n",
    "    def __init__(self, inSide,hiddenSize):\n",
    "        super().__init__()\n",
    "        #self.in = nn.Linear()\n",
    "        self.inputLayer = init_oz(nn.Linear(hiddenSize,inSide))\n",
    "        self.hiddenLayer = init_oz(nn.Linear(hiddenSize,1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.inputLayer(x)\n",
    "        print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = init_oz(nn.Linear(input_size, hidden_size))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = init_oz(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create an instance of the MLP class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unhook(t):\n",
    "    return t.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size=100, hidden_size=60, output_size=1)#opt = Optimizer(gw.parameters(),lr=0.001)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_func = nn.MSELoss()\n",
    "tr_dataloader, te_dataloader = setup_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(1, model, loss_func, opt, tr_dataloader, te_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=torch.optim.SGD): fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb,self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n)\n",
    "        self.ns.append(n)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num,self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs,self.losses,self.ns = [],[],[]\n",
    "        self.model.to(def_device)\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

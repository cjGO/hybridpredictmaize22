{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fe0df-2fd5-42e3-b798-3018262753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hybridpredictmaize22.hybridpredictmaize22.snpCompression import *\n",
    "from hybridpredictmaize22.GEMlearn import *\n",
    "from hybridpredictmaize22.GEMdataset import *\n",
    "from hybridpredictmaize22.snpCompression import *\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import fastcore.all as fc\n",
    "from collections.abc import Mapping\n",
    "from pathlib import Path\n",
    "from operator import attrgetter,itemgetter\n",
    "from functools import partial\n",
    "from copy import copy\n",
    "from contextlib import contextmanager\n",
    "from warnings import warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0280c5b-a584-4a8e-aaae-d01f83089887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "\n",
    "#| export\n",
    "class newGemDataset():\n",
    "    \"\"\"\n",
    "    Pytorch Dataset which can be used with dataloaders for simple batching during training loops\n",
    "    \"\"\"\n",
    "    def __init__(self,W,Y,G, def_device='cpu'):\n",
    "        self.W = W\n",
    "        self.SNP = G\n",
    "        self.Y = Y\n",
    "        self.device = def_device\n",
    "        \n",
    "    def __len__(self): return self.Y[0].shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "      y = self.Y[0][idx]\n",
    "      e = self.Y[1][idx]\n",
    "      h = self.Y[2][idx]\n",
    "\n",
    "      #weather\n",
    "      w = self.W[1][np.where(self.W[0] == e)[0][0]]\n",
    "\n",
    "      #snp\n",
    "      g = snp_data[1][:,np.where(snp_data[0] == h)[0][0]]\n",
    "      return y,g,w\n",
    "\n",
    "\n",
    "#| export\n",
    "class ST():\n",
    "    \"\"\"\n",
    "    A class which will hold the secondary trait data for the entire dataset for pre-training purposes\n",
    "    \n",
    "    init\n",
    "        yield_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, yield_data, testYear):\n",
    "\n",
    "        self.Te = yield_data.iloc[([str(testYear) in x for x in yield_data['Env']]),:].reset_index()\n",
    "        self.Tr = yield_data.iloc[([str(testYear) not in x for x in yield_data['Env']]),:].reset_index()\n",
    "\n",
    "        self.secondary_traits = [\n",
    "               # 'Stand_Count_plants',\n",
    "               # 'Pollen_DAP_days',\n",
    "               # 'Silk_DAP_days',\n",
    "               # 'Plant_Height_cm',\n",
    "               # 'Ear_Height_cm',\n",
    "                #'Root_Lodging_plants',\n",
    "                #'Stalk_Lodging_plants',\n",
    "               # 'Twt_kg_m3',\n",
    "                'Yield_Mg_ha',\n",
    "                #'Date_Harvested'\n",
    "                ]\n",
    "        \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_arrays(self.Tr)\n",
    "        self.make_arrays(self.Te, False)\n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(np.array(self.Tr[self.secondary_traits]))\n",
    "        self.scaler = ss\n",
    "\n",
    "    def scale_data(self,df):\n",
    "        scaled_secondary = self.scaler.transform(np.array(df[self.secondary_traits]))\n",
    "        for c,i in enumerate(self.secondary_traits):\n",
    "            #print(i)\n",
    "            df[i] = scaled_secondary[:,c]\n",
    "    \n",
    "    def plot_yields(self):\n",
    "        for i in self.secondary_traits:\n",
    "            plt.hist(self.Tr[i],density=True, label='Train',alpha=.5,bins=50)\n",
    "            plt.hist(self.Te[i],density=True, label='Test',alpha=.5,bins=50)\n",
    "            plt.legend()\n",
    "            plt.title(i)\n",
    "            plt.show()\n",
    "\n",
    "    def make_arrays(self,df,train=True):\n",
    "      df = np.array(df[self.secondary_traits]), np.array(df['Env']) , np.array(df['Hybrid']), np.array(df['Date_Planted'])\n",
    "      if train:\n",
    "        self.Tr = df\n",
    "      else:\n",
    "        self.Te= df\n",
    "\n",
    "#| export\n",
    "class newWT():\n",
    "    \"\"\"\n",
    "    A class which will hold the weather data for the entire dataset for training purposes\n",
    "    \n",
    "    init\n",
    "        weather_data -> pandas table\n",
    "        testYear -> e.g. 2019. this will set all data from a given year as the Test Set\n",
    "    \"\"\"\n",
    "    def __init__(self, weather_data, testYear):\n",
    "        \n",
    "        self.Te = weather_data.iloc[([str(testYear) in x for x in weather_data['Year']]),:].reset_index()\n",
    "        self.Tr = weather_data.iloc[([str(testYear) not in x for x in weather_data['Year']]),:].reset_index()\n",
    "            \n",
    "        self.setup_scaler()\n",
    "        self.scale_data(self.Tr)\n",
    "        self.scale_data(self.Te)\n",
    "\n",
    "        self.make_array(self.Tr)\n",
    "        self.make_array(self.Te,False)\n",
    "            \n",
    "    def setup_scaler(self):\n",
    "        ss = MinMaxScaler()\n",
    "        ss.fit(self.Tr.select_dtypes('float'))\n",
    "        self.scaler = ss\n",
    "            \n",
    "    def scale_data(self, df):\n",
    "        fd = df.select_dtypes('float')\n",
    "        fs = self.scaler.transform(fd)\n",
    "        df[fd.columns] = fs\n",
    "\n",
    "    def make_array(self, df,train = True):\n",
    "      for c,i in enumerate(set(df['Env'])):\n",
    "        env_weather = np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "        #print(env_weather.shape)\n",
    "        if c == 0:\n",
    "          env_order = list([i])\n",
    "          weather_array =   np.array(df[df['Env'] == i].iloc[:,4:-1])\n",
    "          weather_array = np.expand_dims(weather_array,axis=0)\n",
    "        else:\n",
    "          weather_array = np.vstack((weather_array, env_weather[None,:,:]))\n",
    "          env_order.append(i)\n",
    "\n",
    "        if train:\n",
    "          self.Tr = (np.array(env_order), np.array(weather_array))\n",
    "        else:\n",
    "          self.Te = (np.array(env_order), np.array(weather_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c193b3-ec51-461f-af89-83aadc1a5201",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = 2019\n",
    "test_year=2019\n",
    "\n",
    "path_snps = Path('./data/snpCompress/')\n",
    "data_path = Path('./data/Training_Data/')\n",
    "path_train_weatherTable =data_path/'4_Training_Weather_Data_2014_2021.csv'\n",
    "path_train_yieldTable = data_path/'1_Training_Trait_Data_2014_2021.csv'\n",
    "snp_compression = 'PCS_100'\n",
    "batch_size = 64\n",
    "\n",
    "snp_data = collect_snps(Path('./data/snpCompress/PCS_50/')) # Read in the SNP profiles\n",
    "yield_data = pd.read_csv(path_train_yieldTable) # Read in trait data \n",
    "yield_data = yield_data[yield_data['Yield_Mg_ha'].notnull()] #Remove plots w/ missing yields\n",
    "weather_data = pd.read_csv(path_train_weatherTable) # Read in Weather Data\n",
    "weather_data['Year'] = [x.split('_')[1] for x in weather_data['Env']] #Store Year in a new column\n",
    "#removes yield data where no weather data\n",
    "setYield = set(yield_data['Env'])\n",
    "setWeather = set(weather_data['Env'])\n",
    "only_yield = setYield - setWeather\n",
    "only_weather = setWeather - setYield\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Env']],:]\n",
    "#removes yield data where no genotype data\n",
    "setSNP = set(snp_data[0])\n",
    "setYield = set(yield_data['Hybrid'])\n",
    "only_yield = setYield - setSNP\n",
    "yield_data = yield_data.iloc[[x not in only_yield for x in yield_data['Hybrid']],:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d5346-d620-41f8-ab05-18982ccc1095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16240a53-f1b0-4338-81d6-39c0a5db46ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4227bd7-0228-49bd-ae8a-35372269918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet2(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet2, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, groups=128, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1, groups=128, bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class EfficientNet3(nn.Module):\n",
    "    def __init__(self, in_chan = 1 , num_classes=100):\n",
    "        super(EfficientNet3, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_chan, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the depthwise separable convolutional layers\n",
    "        self.dwconv1 = nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dwconv2 = nn.Conv1d(128, 128, kernel_size=3, stride=2, padding=1,  bias=False)\n",
    "        self.bn5 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # Define the fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolutional layers\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Pass the input through the depthwise separable convolutional layers\n",
    "        x = self.dwconv1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.dwconv2(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Flatten the feature maps before passing them through the fully connected layer\n",
    "        # x = x.view(-1, 128)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class NNEnsemble3(torch.nn.Module):\n",
    "  def __init__(self, hidden_list, models_list, alpha=None):\n",
    "    super().__init__()\n",
    "\n",
    "    self.models = models_list\n",
    "\n",
    "    self.layers = nn.ModuleList()\n",
    "    self.lin1 = torch.nn.Linear(hidden_list[0], hidden_list[1])\n",
    "    self.lin2 = torch.nn.Linear(hidden_list[1], hidden_list[2])\n",
    "    self.out = torch.nn.Linear(hidden_list[2], 1)\n",
    "\n",
    "    \n",
    "    if alpha != None:\n",
    "        print('X')\n",
    "        for c,l in enumerate(self.layers):\n",
    "            print(l)\n",
    "            torch.nn.init.xavier_normal_(l.weight,gain=alpha)\n",
    "    self.out = nn.LazyLinear(1)\n",
    "\n",
    "  def forward(self,x):\n",
    "    g,w = x\n",
    "    g = self.models[0](g)\n",
    "    w = self.models[1](w)\n",
    " #   print(g.shape, w.shape)\n",
    "\n",
    "    if w.dim() == 3:\n",
    "      w = w.view(w.shape[0], w.shape[1] * w.shape[2])\n",
    "    if g.dim() == 3:\n",
    "      g = g.view(g.shape[0], g.shape[1] * g.shape[2])\n",
    "\n",
    "    x = torch.concat((g,w),axis=1)\n",
    "    for c,layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "      x = torch.nn.functional.relu(x) \n",
    "    return self.out(x)\n",
    "\n",
    "def moving_average(arr, window_size):\n",
    "    \"\"\"Calculate the moving average of an array.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (np.ndarray): Input array with shape (n_samples,).\n",
    "    window_size (int): Size of the moving window.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Moving average of the array, with shape (n_samples - window_size + 1,).\n",
    "    \"\"\"\n",
    "    # Initialize a NumPy array to store the moving averages\n",
    "    ma = np.zeros(len(arr) - window_size + 1)\n",
    "    \n",
    "    # Calculate the moving average\n",
    "    for i in range(len(ma)):\n",
    "        ma[i] = np.mean(arr[i:i+window_size])\n",
    "        \n",
    "    return ma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f6903-d6ac-4e52-b934-8e2a0c74e604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_6602/2669084330.py\", line 21, in __getitem__\n    d = self.Y[3][idx]\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#train loop\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m y,g,w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtr_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m  y[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1376\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/burbank/miniconda3/envs/fastai/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_6602/2669084330.py\", line 21, in __getitem__\n    d = self.Y[3][idx]\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gm = EfficientNet3(num_classes=1500)\n",
    "wm = EfficientNet3(300, num_classes=1500)\n",
    "                    \n",
    "\n",
    "model = NNEnsemble3([2000,1000,1000], [gm,wm],alpha=2)# plt.hist(detach_list(model((g,w))))\n",
    "\n",
    "opt = optim.Adamax(model.parameters(), lr= .00005, weight_decay=.0001)\n",
    "#scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=200, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-06)\n",
    "loss_func = torch.nn.functional.huber_loss\n",
    "\n",
    "tr_loss = [] \n",
    "te_loss = []\n",
    "te_MA = []\n",
    "predicts = []\n",
    "targets = []\n",
    "\n",
    "#for i in tqdm(range(len(tr_dataloader))):\n",
    "for i in tqdm(range(2000)):\n",
    "  model.train()\n",
    "  #train loop\n",
    "  y,g,w = next(iter(tr_dataloader))\n",
    "  y =  y[:,-1]\n",
    "  y = y.type(torch.float32).to('cuda')\n",
    "  g = g.type(torch.float32).to('cuda')\n",
    "  w = w.type(torch.float32).to('cuda')\n",
    "\n",
    "  preds = model((g,w))\n",
    "  preds = preds.squeeze(1)\n",
    "\n",
    "  loss = loss_func(y, preds)\n",
    "\n",
    "  loss.backward()\n",
    "  opt.step()\n",
    "  opt.zero_grad()\n",
    "  tr_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30a77b-8d23-46ec-87fa-9d0eef5b32b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
